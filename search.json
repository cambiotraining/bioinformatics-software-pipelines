[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Managing Bioinformatics Software and Pipelines",
    "section": "",
    "text": "Overview\nSetting up a computer for running bioinformatic analysis can be a challenging process. Most bioinformatic applications involve the use of many different software packages, which are often part of long data processing pipelines. These materials cover how you can overcome these challenges by using ’‘’package managers’’’ and ’‘’workflow management software’’’. You will learn about common tools to install software, with a particular focus on the popular conda/mamba package manager. We also cover how you can use pre-existing software containers (Singularity), which are a way to further abstract software requirements by bundling them into virtual environments. Finally, you will learn how to use automated pipelines to streamline your bioinformatic analysis. We focus on the Nextflow software, introducing you to its core pipelines and how you can configure it to run at scale on HPC clusters.",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Managing Bioinformatics Software and Pipelines",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\n\nRecognise the utility of package managers in bioinformatics.\nUse package managers to create and maintain complex software environments.\nDescribe how containerisation solutions can be used to solve the problem of software dependencies.\nInstall, configure and run automated analysis pipelines developed and maintained by the bioinformatics community.\n\n\n\n\nTarget Audience\nThis course is aimed at researchers who are just starting to run bioinformatics analysis on their own. It may be particularly useful if you attended previous training on specific applications (e.g. RNA-seq, ChIP-seq, variant calling, etc.), but are struggling on how to setup and start your analysis. It is also useful if you are using a HPC cluster and would like to learn how to manage software and automate and parallelise your analysis using available pipelines.\nNote that the focus of this course is to introduce these tools from a hands-on and practical user perspective, not as a developer. Therefore, we will not teach you how to write your own pipelines, or create your own software containers or installation recipes.\nWe will also not cover the details of any specific type of bioinformatic analysis. The idea of this course is to introduce the computational tools to get your work done, not to teach how those tools work.\n\n\nPrerequisites\n\nUnix command line (required): you should be comfortable using the command line to navigate your filesystem and understand the basic structure of a command.\nNGS data analysis (desirable): if you are familiar with the basic analysis of a specific type of NGS data (e.g. RNA-seq, ChIP-seq, WGS), it will help you to engage with some of the examples used in the course. However, you can attend this course even if you haven’t done any of those analysis before.\n\n\n\n\nExercises\nExercises in these materials are labelled according to their level of difficulty:\n\n\n\n\n\n\n\nLevel\nDescription\n\n\n\n\n  \nExercises in level 1 are simpler and designed to get you familiar with the concepts and syntax covered in the course.\n\n\n  \nExercises in level 2 combine different concepts together and apply it to a given task.\n\n\n  \nExercises in level 3 require going beyond the concepts and syntax introduced to solve new problems.",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#citation-authors",
    "href": "index.html#citation-authors",
    "title": "Managing Bioinformatics Software and Pipelines",
    "section": "Citation & Authors",
    "text": "Citation & Authors\nPlease cite these materials if:\n\nYou adapted or used any of them in your own teaching.\nThese materials were useful for your research work. For example, you can cite us in the methods section of your paper: “We carried our analyses based on the recommendations in YourReferenceHere”.\n\n\nYou can cite these materials as:\n\nManzano, R., Reid, A., Tavares, H. (2024). Managing bioinformatics software and pipelines. https://cambiotraining.github.io/bioinformatics-software-pipeline/\n\nOr in BibTeX format:\n@misc{YourReferenceHere,\n  author = {Manzano, Raquel and Reid, Adam and Tavares, Hugo},\n  month = {9},\n  title = {Managing bioinformatics software and pipelines},\n  url = {https://cambiotraining.github.io/bioinformatics-software-pipeline/},\n  year = {2024}\n}\nAbout the authors:\nRaquel Manzano  \nAffiliation: Cancer Research UK Cambridge Institute Roles: writing - original content; conceptualisation; software\n\nAdam Reid  \nAffiliation: Gurdon Institute Roles: writing - original content; conceptualisation; software\n\nHugo Tavares  \nAffiliation: Cambridge Centre for Research Informatics Training Roles: writing - original content; conceptualisation; software",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Data & Setup",
    "section": "",
    "text": "Data\nYou can download the data used in these materials as a zip file from dropbox.\nDownload\nAlternatively, using the command line (from within a directory of your choice, e.g. ~/Desktop):",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#data",
    "href": "setup.html#data",
    "title": "Data & Setup",
    "section": "",
    "text": "wget -O biopipelines.zip \"https://www.dropbox.com/scl/fo/hof2gmftr8iq1744jkwnd/AOE_vgyKxphI63OAA5dCzqM?rlkey=k929x9ipitvuexrf72u7hbth9&st=muryirlr&dl=1\"\nunzip biopipelines.zip -d biopipelines_course\nrm biopipelines.zip",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#setup",
    "href": "setup.html#setup",
    "title": "Data & Setup",
    "section": "Setup",
    "text": "Setup\n\nConda\nWe recommend using the Conda package manager to install your software. In particular, the newest implementation called Mamba.\nTo install Mamba, run the following commands from the terminal:\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh -b -p $HOME/miniforge3\nrm Miniforge3-$(uname)-$(uname -m).sh\n$HOME/miniforge3/bin/mamba shell init\nRestart your terminal (or open a new one). Then run the following commands:\nconda config --add channels nodefaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\nconda config --set remote_read_timeout_secs 1000\n\n\nNextflow\nWe recommend that you install Nextflow within a conda environment. You can do that with the following command:\nmamba create -n nextflow -y nextflow\nWhen you want to use Nextflow make sure to activate this software environment by running mamba activate nextflow.\n\n\nSingularity\nWe recommend that you install Singularity and use the -profile singularity option when running Nextflow pipelines. On Ubuntu/WSL2, you can install Singularity using the following commands:\nsudo apt install -y libfuse2t64 runc fuse2fs uidmap\nwget -O singularity.deb https://github.com/sylabs/singularity/releases/download/v4.3.7/singularity-ce_4.3.7-$(lsb_release -cs)_amd64.deb\nsudo dpkg -i singularity.deb\nrm singularity.deb\nIf you have a different Linux distribution, you can find more detailed instructions on the Singularity documentation page.\nIf you have issues running Nextflow pipelines with Singularity, then you can follow the instructions below for Docker instead.",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "materials/01-package_managers.html",
    "href": "materials/01-package_managers.html",
    "title": "3  Package Managers",
    "section": "",
    "text": "3.1 What is a package manager?\nMost operating systems have package managers available, which allow the user to manage (install, remove, upgrade) their software easily. The package manager takes care of automatically downloading and installing the software we want, as well as any dependencies it requires.\nThere are many package managers available, some are specific to a given type of operating system, or specific to a programming language, while others are more generic. Each of these package managers will use their own repositories, meaning they have access to different sets of software (although there is often some overlap). Some examples include:\nSome programming languages also come with their own package managers. For example:\nIn many cases package managers can also install software directly from code repositories such as GitHub, adding further flexibility to how we manage our scientific software.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Managers</span>"
    ]
  },
  {
    "objectID": "materials/01-package_managers.html#what-is-a-package-manager",
    "href": "materials/01-package_managers.html#what-is-a-package-manager",
    "title": "3  Package Managers",
    "section": "",
    "text": "Diagram illustrating how package managers work. Image from It’s FOSS, licensed under CC BY-SA 4.0\n\n\n\n\napt is the default Linux package manager for Debian-derived distributions, such as the popular Ubuntu. It comes pre-installed and can be used to install system-level applications.\nhomebrew is a popular package manager for macOS, although it also works on Linux.\nconda/mamba is a package manager very popular in bioinformatics and data science communities, due to the repositories which give access to software used in these fields. It will be the main focus of this section.\n\n\n\nThe statistical software R has two main library repositories: CRAN and Bioconductor. These are installed from within the R console using the commands install.packages() and BiocManager::install(), respectively.\nThe programming laguage Python has a package manager called pip, which has access to the Python Package Index (PyPI) repository.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Managers</span>"
    ]
  },
  {
    "objectID": "materials/01-package_managers.html#condamamba",
    "href": "materials/01-package_managers.html#condamamba",
    "title": "3  Package Managers",
    "section": "3.2 Conda/Mamba",
    "text": "3.2 Conda/Mamba\nA popular package manager in data science, scientific computing and bioinformatics is Mamba, which is a successor to another package manager called Conda.\nConda was originally developed by Anaconda as a way to simplify the creation, distribution, and management of software environments containing different packages and dependencies. It is known for its cross-platform compatibility and relative ease of use (compared to compiling software and having the user manually install all software dependencies). Mamba is a more recent and high-performance alternative to Conda. While it maintains compatibility with Conda’s package and environment management capabilities, Mamba is designed for faster dependency resolution and installation, making it a better choice nowadays. Therefore, the rest of this section focuses on Mamba specifically.\nOne of the strengths of using Mamba to manage your software is that you can have different versions of your software installed alongside each other, organised in environments. Organising software packages into environments is extremely useful, as it allows to have a reproducible set of software versions that you can use and reuse in your projects.\nFor example, imagine you are working on two projects with different software requirements:\n\nProject A: requires Python 3.7, NumPy 1.15, and scikit-learn 0.20.\nProject B: requires Python 3.12, the latest version of NumPy, and TensorFlow 2.0.\n\nIf you don’t use environments, you would need to install and maintain these packages globally on your system. This can lead to several issues:\n\nVersion conflicts: different projects may require different versions of the same library. For example, Project A might not be compatible with the latest NumPy, while Project B needs it.\nDependency chaos: as your projects grow, you might install numerous packages, and they could interfere with each other, causing unexpected errors or instability.\nDifficulty collaborating: sharing your code with colleagues or collaborators becomes complex because they may have different versions of packages installed, leading to compatibility issues.\n\n\n\n\nIllustration of Conda/Mamba environments. Each environment is isolated from the others (effectively in its own folder), so different versions of the packages can be installed for distinct projects or parts of a long analysis pipeline.\n\n\nMamba allows you to create self-contained software environments for each project, addressing these issues:\n\nIsolation: you can create a separate environment for each project. This ensures that the dependencies for one project don’t affect another.\nSoftware versions: you can specify the exact versions of libraries and packages required for each project within its environment. This eliminates version conflicts and ensures reproducibility.\nEase of collaboration: sharing your code and environment file makes it easy for collaborators to replicate your environment and run your project without worrying about conflicts.\nSimplified maintenance: if you need to update a library for one project, it won’t impact others. You can manage environments separately, making maintenance more straightforward.\n\nAnother advantage of using Mamba is that the software is installed locally (by default in your home directory), without the need for admin (sudo) permissions.\n\n\n\n\n\n\nNoteHow do I install Mamba?\n\n\n\nWe give instructions to install Mamba on our setup page.\n\n\n\n3.2.1 Installing software with Mamba\nYou can search for available packages from the anaconda.org website. Packages are organised into “channels”, which represent communities that develop and maintain the installation “recipes” for each software. The most popular channels for bioinformatics and data analysis are “bioconda” and “conda-forge”.\nThere are three main commands to use with Mamba:\n\nmamba create -n ENVIRONMENT-NAME: this command creates a new software environment, which can be named as you want. Usually people name their environments to either match the name of the main package they are installing there (e.g. an environment called pangolin if it’s to install the Pangolin software). Or, if you are installing several packages in the same environment, then you can name it as a topic (e.g. an environment called rnaseq if it contains several packages for RNA-seq data analysis).\nmamba install -n ENVIRONMENT-NAME  NAME-OF-PACKAGE: this command installs the desired package in the specified environment.\nmamba activate ENVIRONMENT-NAME: this command “activates” the environment, which means the software installed there becomes available from the terminal.\n\nLet’s see a concrete example. If we wanted to install packages for phylogenetic analysis, we could do:\n# create an environment named \"phylo\"\nmamba create -n phylo\n\n# install some software in that environment\nmamba install -n phylo iqtree==2.3.3 mafft==7.525\nIf we run the command:\nmamba env list\nWe will get a list of environments we created, and “phylo” should be listed there. If we want to use the software we installed in that environment, then we can activate it:\nmamba activate phylo\nAnd usually this changes your terminal to have the word (phylo) at the start of your prompt instead of (base).\n\n\n3.2.2 Environment files\nAlthough we can create and manage environments as shown above, it may sometimes be useful to specify an environment in a file. This is particularly useful if you want to document how your environment was created and if you want to recreate it somewhere else.\nEnvironments can be defined using a specification file in YAML format (a simple text format often used for configuration files). For example, our phylogenetics environment above could be specified as follows:\nname: phylo\nchannels:\n  - conda-forge\n  - bioconda\ndependencies:\n  - iqtree==2.3.3\n  - mafft==7.525\nWe have included this example in the file demo/envs/phylo.yml. To create the environment from the file, we can use the command:\nmamba env create -f envs/phylo.yml\nNote that this command is slightly different from the one we saw earlier: mamba env create -f environment.yml as shown here is to create an environment from a file, whereas mamba create -n name-of-environment that we saw earlier is used to create a new environment from scratch.\nIf you later decide to update the environment, either by adding a new software or by updating the software versions, you can run the command:\nmamba env update -f envs/phylo.yml\nYou can practice this in an exercise below.\n\n\n\n\n\n\nNoteCreate a YAML file from an existing environment\n\n\n\nIf you did not create an environment file at the start of your project, you can create one from an existing environment using the command mamba env export &gt; env.yaml\n\n\n\n\n3.2.3 Mixing package managers\nThere might be times when some packages/libraries are not available directly through conda/mamba. For example, there might be a python library that is only available through pip. Or an R package that you want to install from GitHub. There are ways to address these challenges, which we cover below.\n\npip packages\nYou can use pip to install packages within a Conda environment. You should be careful when doing this, as pip may change your conda-installed packages, which might break the conda environment. There are a few steps one can follow to avoid this pitfall:\n\nStart from a new and clean environment. If the new environment breaks you can safely remove it and start over.\nInstall pip in your conda environment. This is important as the pip you have in your base environment is different from your new environment (will avoid conflicts).\nInstall any conda packages your need to get the environment ready and leave the pip install for last. Avoid switching between package managers. Start with one and finish with the other one so reversing or fixing conflicts is easier.\n\nThe Anaconda blog has a useful best-practices checklist covering these points.\nThe pip-installed packages can be specified in the YAML file. For example, let’s say we wanted to install the package nomspectra for working with high-resolution mass spectrometry data. This package is available from PiPy, but not on any public Conda channel. It requires Numpy version 1, so we may specify our environment like this:\nname: massspec\nchannels: \n  - conda-forge\n  - bioconda\ndepedencies: \n  - numpy=1.22.4\n  - pip:\n    - nomspectra\nNotice the new syntax, where we specify packages we want installed with pip at the end of the configuration file.\n\n\nR packages\nMost R and Bioconductor packages are available through the conda-forge and bioconda channels. However, you may be interested in installing a non-published package directly from GitHub/GitLab/Bitbucket. In R, this can be done using the remotes package (or, alternatively, devtools), which contains a function install_github() allowing you to do this.\nUnfortunately, there is no way to specify installing such packages directly in an environment YAML file. The workaround in this case is to perform things in two steps:\n\nCreate a new environment with R, any packages you want and that are available from Conda channels, and the package remotes.\nActivate the environment.\nLaunch an R console directly from the terminal.\nRun the command install_github()/install_gitlab()/install_bitbucket() as appropriate.\n\nLet’s see an example of using the package qtl2helper (available on github), which contains addon functions to work with objects from the qtl2 package. The qtl2 package itself, is available from the usual CRAN and, as such, also available from conda-forge. Let’s say in addition we also wanted the tidyverse package. We could create the following YAML:\nname: qtl\nchannels:\n  - conda-forge\n  - bioconda\ndependencies: \n  - r-qtl2=0.36\n  - r-tidyverse=2.0.0\n  - r-remotes=2.5.0\n\n# Manually installed qtl2helper in this environment with\n# remotes::install_github(\"tavareshugo/qtl2helper\")\nWe install the dependencies that are available from Conda, and added a comment to our YAML to indicate we further installed packages “manually”. We could do so by activating our environment (mamba activate qtl), launching an R terminal and then running the command shown.\n\n\n\n\n\n\nWarningManaging R packages\n\n\n\nOn your local computer, we usually recommend that you manage your R packages normally, without the use of Conda. However, it may sometimes be necessary to setup R environments on HPC servers, in which case the method above would work.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Managers</span>"
    ]
  },
  {
    "objectID": "materials/01-package_managers.html#disadvantages-and-pitfalls",
    "href": "materials/01-package_managers.html#disadvantages-and-pitfalls",
    "title": "3  Package Managers",
    "section": "3.3 Disadvantages and pitfalls",
    "text": "3.3 Disadvantages and pitfalls\n\nDependency conflicts\nOne thing to be very careful about is how Conda/Mamba manages the dependency graph of packages to install. If you don’t specify the version of the software you want, in theory Mamba will pick the latest version available on the channel. However, this is conditional on the other packages that are installed alongside it, as some versions may be incompatible with each other, it may downgrade some packages without you realising.\n\nTake this example, where we create a new environment called metagen for a metagenomics project. We initiate the environment with only two packages: GTDB-tk (taxonomic classification of genomes) and MultiQC (quality control reporting tool):\nmamba create -n metagen multiqc gtdbtk\nWhen you run this command, Mamba will ask if you want to proceed with the installation. Before proceeding, it’s always a good idea to check which versions of the packages we are interested in are being installed.\nAt the time of writing, the latest version of GTDB-tk on anaconda.org is 2.4.0, however as we run this command we can see that Mamba is installing version 2.3.0, which is a version behind the latest.\nLet’s be more explicit and specify we want the latest versions available for both packages (at the time of writing):\nmamba create -n metagen multiqc==1.28 gtdbtk==2.4.0\nBy running this command, we get an error message informing us that Mamba could not find a fully compatible environment for these software versions. The message is very long, we only show the top few lines:\nCould not solve for environment specs\nThe following packages are incompatible\n├─ gtdbtk 2.4.0  is installable with the potential options\n│  ├─ gtdbtk 2.4.0 would require\n│  │  ├─ fastani 1.32.* , which requires\n│  │  │  └─ boost &gt;=1.70.0,&lt;1.70.1.0a0  with the potential options\n│  │  │     ├─ boost 1.70.0 would require\n│  │  │     │  └─ python &gt;=2.7,&lt;2.8.0a0 , which can be installed;\n│  │  │     ├─ boost 1.70.0 would require\n│  │  │     │  └─ python &gt;=3.6,&lt;3.7.0a0 , which can be installed;\n│  │  │     ├─ boost 1.70.0 would require\n│  │  │     │  └─ python &gt;=3.7,&lt;3.8.0a0  with the potential options\n│  │  │     │     ├─ python [3.7.0|3.7.1|...|3.7.9], which can be installed;\n│  │  │     │     └─ python [3.7.10|3.7.12] would require\n│  │  │     │        └─ python_abi 3.7.* *_cp37m, which can be installed;\n\n... etc ...\nThe message is a bit hard to interpret, but generally we can see that the issue seems to be related to the Python versions required by these two packages.\nHow could we solve this problem? One possibility is to install each software in a separate environment. The disadvantage is that you will need to run several mamba activate commands at every step of your analysis.\nAnother possibility is to find a compatible combination of package versions that is sufficient for your needs. For example, let’s say that GTDB-tk was the most critical software for which we needed to run the latest version. We could find what is the latest version of MultiQC compatible with it, by forcing the GTDB-tk version, but not the other one:\nmamba create -n metagen multiqc gtdbtk==2.4.0\nRunning this command, we can see that we would get multiqc==1.21. So, MultiQC would be a slightly older version than currently available, but for our purposes this might not be a problem. If we were happy with this choice, then we could proceed. For reproducibility, we could save all this information in a YAML file specifying our environment:\nname: metagen\nchannels:\n  - conda-forge\n  - bioconda\ndependencies:\n  - multiqc==1.21\n  - gtdbtk==2.4.0\n\n\nPackage availability\nSome packages not available:\n\nCell Ranger is a very popular software for processing single-cell RNA-seq data from the 10x genomics platform. However, the software is not open source and therefore not available through the bioconda channel.\nSoftware that is not used by a wide-enough community, and thus has no available installation recipe. For example AliView (to visualise multiple sequence alignments) or APAtrap (differential usage of alternative polyadenylation sites from RNA-seq).\n\n\n\nDisk space\nEnvironments can take a lot of disk space in your system. This is software-dependent, but in some cases can become quite substantial (several GB of files). Therefore, it’s good practice to:\n\nRemove unused environments: regularly check for environments that you no longer need and remove them using mamba env remove --name ENV_NAME. You can use mamba env list to list your environments and find unused ones.\nRecreate environments from YAML files: related to the previous point, always make sure to keep YAML enviroment files for your environments. This way, you can safely remove less frequently used environments and later recreate them using mamba env create -f env.yml. If you did not create an environment file, you can create one from an existing environment with mamba env export &gt; env.yaml.\nRegularly clear cached packages: Mamba caches downloaded packages for faster installation in the future. However, you can clear this cache using mamba clean --all. This is particularly useful after upgrades, as you don’t need old versions of the packages stored in your system.\n\nTo see if this is a problem in your system, you can occasionally check the size of your Miniforge installation folder with the following command (assuming default installation path):\ndu --si -s $CONDA_PREFIX\nNote the $CONDA_PREFIX is an environment variable that stores the directory path to your conda/mamba installation.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Managers</span>"
    ]
  },
  {
    "objectID": "materials/01-package_managers.html#exercises",
    "href": "materials/01-package_managers.html#exercises",
    "title": "3  Package Managers",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\n\n\n\n\n\n\nExerciseExercise 1 - Creating a new Mamba environment\n\n\n\n\n\n\nGo to the demo directory, where you will find some FASTQ files in the reads folder. The objective in this exercise is to setup a software environment to run a standard quality control software on these sequencing reads.\n\nUse a text editor to create a Conda/Mamba environment file called envs/qc.yml. This file should specify:\n\nEnvironment name: qc\nChannels: conda-forge, bioconda\nPackages: FastQC v0.12.1 and MultiQC v1.21 (check available packages at anaconda.org).\n\nUsing mamba build the environment from your created file.\nActivate your new environment and run the QC script provided: bash scripts/01-qc.sh (you can look inside the script to see what it is doing).\nCheck if you obtained the final output file in results/qc/multiqc_report.html.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe can see how to specify an environment file manually on the Conda documentation page. Following those instructions, we have created the following file and saved it as envs/qc.yml:\nname: qc\nchannels:\n  - conda-forge\n  - bioconda\ndependencies:\n  - fastqc==0.12.1\n  - multiqc==1.21\nWe then created our environment with the command:\nmamba env create -f envs/qc.yml\nWe then activate our environment:\nmamba activate qc\nAnd finally ran the script provided:\nbash scripts/01-qc.sh\nWe can see the script ran successfully by looking at the output directory results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseExercise 2 - Update a Mamba environment\n\n\n\n\n\n\nGoing back to the envs/phylo.yml environment (in the demo folder), update the environment to include a software to for dating phylogenetic trees called TreeTime.\n\nGo to anaconda.org to see what is the latest version available and from which channel.\nUpdate the YAML environment file to include it.\nUpdate the environment.\nCheck if the software was installed successfully by running treetime --version.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe can see the software is available from https://anaconda.org/bioconda/treetime, provided from the bioconda channel. The latest version at the time of writing is 0.11.3, so it is the one we demonstrate below.\nUsing a text editor of our choice, we update our YAML file:\nname: phylo\nchannels:\n  - conda-forge\n  - bioconda\ndependencies:\n  - iqtree==2.3.3\n  - mafft==7.525\n  - treetime==0.11.3\nAfter saving the changes, we update our environment:\nmamba env update -f envs/phylo.yml\nOnce the update runs successfully, we activate the environment first with mamba activate phylo and then test our software:\ntreetime --version\ntreetime 0.11.3\nThe command runs successfully, with the expected version printed, indicating it is successfully installed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseExercise 3 - Package dependencies\n\n\n\n\n\n\nA PhD student working on a machine learning project is trying to improve a classification model from a previous publication.\nFor benchmarking purposes, the student wants to reproduce the previous analysis and therefore use the same version of the package used by the authors: PyTorch version 1.13.0.\nFor their own improved model, they will use a recent version of a different library: TensorFlow version 2.17.0.\nThe student came to you for advice: they’ve heard of Mamba and would like to create an environment for their project. What would you recommend to them?\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe can start by running a command to create an environment with both packages, making sure we pin the specific versions required (we call the environment ml in this example, but you could name it anything you want):\nmamba create -n ml tensorflow==2.17.0 pytorch==1.13.0\nHowever, we get a package dependency error:\n\nerror    libmamba Could not solve for environment specs\n    The following packages are incompatible\n    ├─ pytorch ==1.13.0 * is requested and can be installed;\n    └─ tensorflow ==2.17.0 * is not installable because it requires\n       └─ tensorflow-base [==2.17.0 cpu_py310h98e3cc3_1|==2.17.0 cpu_py310h98e3cc3_2|...|==2.17.0 cuda120py39hf283d87_203], which requires\n          └─ keras &gt;=3.0 * with the potential options\n             ├─ keras [3.0.2|3.0.4|3.0.5|3.1.0] would require\n             │  └─ tensorflow &gt;=2.15.0,&lt;2.17.0a *, which can be installed;\n             ├─ keras [3.1.0|3.1.1] would require\n             │  └─ pytorch =2.1 *, which conflicts with any installable versions previously reported;\n             ├─ keras [3.10.0|3.11.2|...|3.9.2] would require\n             │  └─ pytorch &gt;=2.1.0 *, which conflicts with any installable versions previously reported;\n             ├─ keras 3.12.0 would require\n             │  └─ pytorch &gt;=2.6.0 *, which conflicts with any installable versions previously reported;\n             └─ keras [3.2.0|3.2.1|3.3.2|3.3.3|3.4.1] would require\n                └─ pytorch &gt;=2.1,&lt;2.3 *, which conflicts with any installable versions previously reported.\ncritical libmamba Could not solve for environment specs\n\nThis is because PyTorch 1.13.0 is quite an old version, while TensorFlow 2.17.0 is quite a new version, leading to different version requirements. In fact, TensorFlow itself depends on PyTorch, but version 2.17 requires more recent versions of PyTorch than 1.13.\nAs there is no easy way to install both of these packages in the same environment, our best recommendation to the student would be to create two separate environments for these packages:\nmamba create -n pytorch pytorch==1.13.0\nmamba create -n tensorflow tensorflow==2.17.0\nIn this case we simply named the environments after the main package we’re installing, but again you could choose different names.\nInstalling the packages in separate environments avoids the previous Python version conflict.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Managers</span>"
    ]
  },
  {
    "objectID": "materials/01-package_managers.html#summary",
    "href": "materials/01-package_managers.html#summary",
    "title": "3  Package Managers",
    "section": "3.5 Summary",
    "text": "3.5 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nA package manager automates the process of installing, upgrading, configuring, and managing software packages, including their dependencies.\nExamples of package managers are pip (Python), apt (Debian/Ubuntu) and conda/mamba (generic).\nDependency conflicts, which often arise in complex bioinformatic workflows, can be resolved by managing software in isolated environments.\nConda/Mamba simplify these tasks by managing dependencies, creating isolated environments, and ensuring reproducible setups across different systems.\nKey Mamba commands include:\n\nmamba create --name ENVIRONMENT-NAME to create a new environment.\nmamba install -n ENVIRONMENT-NAME  NAME-OF-PACKAGE to install a package inside that environment.\nmamba activate ENVIRONMENT-NAME to make the software from that environment available.\nmamba env create -f ENVIRONMENT-YAML-SPECIFICATION to create an environment from a YAML file (recommended for reproducibility).\nmamba env update -f ENVIRONMENT-YAML-SPECIFICATION to update an environment from a YAML file (recommended for reproducibility).\n\nRecognise some of limitations of Mamba as a package manager and how to avoid common pitfalls.\nThere are some disadvantages/limitations of Mamba as a package manager:\n\nDependencies aren’t always respected.\nSoftware versions are sometimes downgraded without explicit warning.\nIt can be slow at resolving very complex environments.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Package Managers</span>"
    ]
  },
  {
    "objectID": "materials/02-containers.html",
    "href": "materials/02-containers.html",
    "title": "4  Container Virtualisation",
    "section": "",
    "text": "4.1 Overview\nSoftware containerisation is a way to package software and its dependencies in a single file. A software container can be thought of as a small virtual machine, with everything needed to run that software stored inside that file. Software containers are self-contained, meaning that they are isolated from the host system. This ensures reproducibility, addressing the issue of incompatible dependencies between tools (similarly to Mamba environments). They can run on a local computer or on a high-performance computing cluster, producing the same result. The same analysis can be run on different systems ensuring consistency and reproducibility.\nFor these reasons, software containerisation solutions, such as Docker and Singularity, are widely used in bioinformatics. While these two container software solutions share many similarities, we will focus our attention on Singularity, as it is more widely used in HPC cluster systems (but it can also be used on a regular computer). However, it’s worth keeping in mind that images created with Docker can be compatible with Singularity and vice versa.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Container Virtualisation</span>"
    ]
  },
  {
    "objectID": "materials/02-containers.html#overview",
    "href": "materials/02-containers.html#overview",
    "title": "4  Container Virtualisation",
    "section": "",
    "text": "NoteDocker vs singularity\n\n\n\n\n\nThere are some key differences between Docker containers and Singularity containers. The most important being the necessary permission level of the containers. Docker containers run as root (admin) by default, which means that they have full access to the host system. While this can be advantageous in some cases, it can also pose security risks, particularly in multi-user environments. Singularity, on the other hand, runs containers as non-root users by default, which can improve security and prevent unauthorized access to the host system. Singularity is specifically designed for use in HPC environments and can run on a wide variety of platforms and systems without root access.\nTL;TR:\n\nDocker is well-suited for building and distributing software across different platforms and operating systems.\nSingularity is specifically designed for use in HPC environments and can provide improved security and performance in those settings.\n\n\n\n\n\n\n\n\n\n\nNoteHave heard about Apptainer?\n\n\n\n\n\nSingularity as we knew it is in fact no longer actively maintained and is deprecated - what happened to it? It was essentially split into two parallel projects:\n\nApptainer: this is a rebranding of the original Singularity, which is supported by the Linux Foundation since 2021 (announcement). The Linux foundation is a non-profit that provides a neutral, trusted hub for developers and organizations to code, manage, and scale open technology projects and ecosystems. As such, Apptainer is and will remain free and open source (FOSS) software. Apptainer is still functionally the same as the original Singularity, so you should be able to use Singularity images with Apptainer.\nSingularityCE: this is a fork of the original Singularity, which is developed and maintained by the commercial company Sylabs.\n\nThere are some differences between Apptainer and SingularityCE, but in practice they work very similarly to each other.\n\n\n\n\n\n\n\n\n\nNoteHow do I install Singularity?\n\n\n\nUsually, Singularity is used on a HPC environment and in that case we recommend that you use the version installed by your system administrators.\nIf you want a local installation, you can install Singularity using Mamba:\nmamba create -n singularity conda-forge::singularity==3.8.7\nYou may want to change the command above to use the lastest version available here.\nAlternatively, we show how to perform a system-level installation in our setup page.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Container Virtualisation</span>"
    ]
  },
  {
    "objectID": "materials/02-containers.html#singularity-images",
    "href": "materials/02-containers.html#singularity-images",
    "title": "4  Container Virtualisation",
    "section": "4.2 Singularity images",
    "text": "4.2 Singularity images\nContainers are constructed from images - these are executable files that generate the container. To help understand this, consider images like a sheet of music and containers as the actual music you hear. The sheet music is portable and can be performed on various instruments, yet the melody remains consistent regardless of where it’s played.\nAlthough you can build your own Singularity images, for many popular software there are already pre-built images available from public repositories. Some popular ones are:\n\nBioContainers: a community project that provides a search page for bioinformatic software images stored on several repositories. This is our recommendation as the first page to look at, as it includes results from several repositories (Quay.io, DockerHub, Galaxy).\nGalaxy Project: an open-source platform for bioinformatic data analysis. They provide many pre-built images on their galaxy depot.\nDockerHub: a generic repository of Docker images, which can also be used with Singularity.\nQuay.io: a generic repository of images maintained by Red Hat.\nSylabs: a generic repository maintained by the Singularity developers.\n\nFor example, let’s consider the SeqKit program, which is a toolkit for manipulating FASTA/Q files. If we search on those websites, we will see this software is available on all of them. In this case, the version on Sylabs (here) is older than the one listed on BioContainers (here).\nLooking at that BioContainers page, we can see the link to the image, which is pulled from depot.galaxyproject.org. To pull the image, we can do the following:\n# create a directory for our singularity images\nmkdir images\n\n# download the image\nsingularity pull images/seqkit-2.8.2.sif https://depot.galaxyproject.org/singularity/seqkit:2.8.2--h9ee0642_1\nHere, we are saving the image file as seqkit-2.8.2.sif (.sif is the standard extension for singularity images). Once we have this image available, we are ready to run the software, which will see in practice with the exercise below.\n\n\n\n\n\n\nTipUsing Docker images with Singularity\n\n\n\nIt’s worth noting that Docker images are also compatible with Singularity. For example, on that same Biocontainers seqkit page, we are given a link to use with the docker pull command. But we can use that same link to build a Singularity image using the docker:// prefix in our link, like so:\nsingularity pull images/seqkit-2.8.2.sif docker://quay.io/biocontainers/seqkit:2.8.2--h9ee0642_1\nThese are effectively the same image, just pulled from two different servers. The image on depot.galaxy.org is already pre-built, so we’re effectively only downloading a file to our computer. On the other hand the image from Quay.io is the “recipe” to build the image, which the Singularity command takes care of building for us.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Container Virtualisation</span>"
    ]
  },
  {
    "objectID": "materials/02-containers.html#executing-commands",
    "href": "materials/02-containers.html#executing-commands",
    "title": "4  Container Virtualisation",
    "section": "4.3 Executing commands",
    "text": "4.3 Executing commands\nTo execute a command inside the image container, we can use the singularity run command. For example, let’s run the command seqkit --help to look at the help documentation of the SeqKit software:\nsingularity run images/seqkit-2.8.2.sif seqkit --help\nAnd this should print the help of the program. Note that SeqKit is not installed on your system, rather the command is avaible inside the container, as it was pre-installed in its image file.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Container Virtualisation</span>"
    ]
  },
  {
    "objectID": "materials/02-containers.html#filesystem-binding",
    "href": "materials/02-containers.html#filesystem-binding",
    "title": "4  Container Virtualisation",
    "section": "4.4 Filesystem binding",
    "text": "4.4 Filesystem binding\nOne important thing to note is that, by default, not your entire filesystem is visible to the container. You have your home directory mounted on the image by default, but nothing else.\nIf you want to operate on files that are on a different file partition, you need to bind them to the container. This is done using the --bind option, which requires a full path to the directory you want to make available on the image.\nFor example, say a user called robin was working on a directory called /scratch/robin/awesomeproject. Because this directory is not in the default home (/home/robin), it will not be available on the container by default. The user could make it available as the default directory by running:\nsingularity run --bind /scratch/robin/awesomeproject images/seqkit-2.8.2.sif seqkit --help\nSee more information about this in the Singularity documentation.\n\n\n\n\n\n\nNoteSingularity on HPC systems\n\n\n\nTypically, Singularity is pre-installed on HPC servers by the system administrators, and we recommend that you use the version installed by your system admins. This is because they will have often already set specific bindings to the filesystem, meaning you don’t need to do this yourself.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Container Virtualisation</span>"
    ]
  },
  {
    "objectID": "materials/02-containers.html#exercises",
    "href": "materials/02-containers.html#exercises",
    "title": "4  Container Virtualisation",
    "section": "4.5 Exercises",
    "text": "4.5 Exercises\n\n\n\n\n\n\nExerciseExercise 1\n\n\n\n\n\n\nTo illustrate the use of Singularity, we will use the seqkit software to extract some basic statistics from the sequencing files in the demo/reads directory. If you haven’t done so already, first download the image with the commands shown above.\nWe can test if our image is working by running the seqkit --help command in our container:\nsingularity run images/seqkit-2.8.2.sif seqkit --help\n\nRun seqkit stats reads/*.fastq.gz using the singularity image we downloaded earlier.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nThe Singularity command is:\nsingularity run images/seqkit-2.8.2.sif seqkit stats reads/*.fastq.gz\nIf we run this, it produces an output like this:\nfile                                     format  type   num_seqs      sum_len  min_len  avg_len  max_len\nreads/SRR7657872_1.downsampled.fastq.gz  FASTQ   DNA   1,465,993  219,898,950      150      150      150\nreads/SRR7657872_2.downsampled.fastq.gz  FASTQ   DNA   1,465,993  219,898,950      150      150      150\nreads/SRR7657874_1.downsampled.fastq.gz  FASTQ   DNA   1,379,595  206,939,250      150      150      150\nreads/SRR7657874_2.downsampled.fastq.gz  FASTQ   DNA   1,379,595  206,939,250      150      150      150\nreads/SRR7657876_1.downsampled.fastq.gz  FASTQ   DNA   1,555,049  233,257,350      150      150      150\nreads/SRR7657876_2.downsampled.fastq.gz  FASTQ   DNA   1,555,049  233,257,350      150      150      150\nreads/SRR7657877_1.downsampled.fastq.gz  FASTQ   DNA   1,663,432  249,514,800      150      150      150\nreads/SRR7657877_2.downsampled.fastq.gz  FASTQ   DNA   1,663,432  249,514,800      150      150      150",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Container Virtualisation</span>"
    ]
  },
  {
    "objectID": "materials/02-containers.html#summary",
    "href": "materials/02-containers.html#summary",
    "title": "4  Container Virtualisation",
    "section": "4.6 Summary",
    "text": "4.6 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nSoftware containerisation solutions such as Docker and Singularity achieve isolation and consistency in running software. This is because they encapsulate applications with their dependencies in a file (known as an image), ensuring consistent performance across different environments.\nThe computer science and bioinformatics community has built many images for commonly used software, including the galaxy depot, Sylabs and dockerhub.\nTo download an image from the online repositories we can use the command singularity pull &lt;URL TO IMAGE&gt;\nTo run a command using an existing image, we use the command: singularity run &lt;PATH TO IMAGE&gt; &lt;YOUR COMMAND&gt;\nWe can mount specific directories in our filesystem to the Singularity container using the --bind option with a full path to the directory we want to make available.",
    "crumbs": [
      "Slides",
      "Software",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Container Virtualisation</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html",
    "href": "materials/03-nfcore.html",
    "title": "5  Automated Workflows",
    "section": "",
    "text": "5.1 Overview\nBioinformatic analyses always involve multiple steps where data is gathered, cleaned and integrated to give a final set of processed files of interest to the user. These sequences of steps are called a workflow or pipeline. As analyses become more complex, pipelines may include the use of many different software tools, each requiring a specific set of inputs and options to be defined. Furthermore, as we want to chain multiple tools together, the inputs of one tool may be the output of another, which can become challenging to manage.\nAlthough it is possible to code such workflows using shell scripts, these often don’t scale well across different users and compute setups. To overcome these limitations, dedicated workflow/pipeline management software packages have been developed to help standardise pipelines and make it easier for the user to process their data. These dedicated packages are designed to streamline and automate the process of coordinating complex sequences of tasks and data processing (for instance an RNA-seq analysis). In this way, researchers can focus on their scientific questions instead of the nitty-gritty of data processing.\nHere are some of the key advantages of using a standardised workflow for our analysis:",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#overview",
    "href": "materials/03-nfcore.html#overview",
    "title": "5  Automated Workflows",
    "section": "",
    "text": "A) Workflow illustration, showing ready-for-running tasks highlighted in green, indicating all necessary input files are available. The initial red task produces a temporary file, potentially removable once the blue tasks complete. Workflow management systems ensure tasks are run in an optimal and automated manner. For example, in B) there is suboptimal scheduling of tasks, as only one blue task is scheduled the temporary file cannot be removed. Conversely, in C) we have optimal scheduling, since the three blue tasks are scheduled first enabling deletion of the temporary file after their completion. Diagram taken from Mölder et al. 2025, licensed under CC BY 4.0).\n\n\n\n\nFewer errors - because the workflow automates the process of managing input/output files, there are less chances for errors or bugs in the code to occur.\nConsistency and reproducibility - analysis ran by different people should result in the same output, regardless of their computational setup.\nSoftware installation - all software dependencies are automatically installed for the user using solutions such as Conda/Mamba, Docker and Singularity.\nScalability - workflows can run on a local desktop or scale up to run on high performance compute (HPC) clusters.\nCheckpoint and resume - if a workflow fails in one of the tasks, it can be resumed at a later time.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#nextflow-and-snakemake",
    "href": "materials/03-nfcore.html#nextflow-and-snakemake",
    "title": "5  Automated Workflows",
    "section": "5.2 Nextflow and Snakemake",
    "text": "5.2 Nextflow and Snakemake\nTwo of the most popular workflow software packages are Snakemake and Nextflow. We will not cover how to develop workflows with these packages, but rather how to use existing workflows developed by the community.1 Both Snakemake and Nextflow offer similar functionality and can work on different computer systems, from personal laptops to large cloud-based platforms, making them very versatile. One of the main noticeable difference to those developing pipelines with these tools is that Snakemake syntax is based on Python, whereas Nextflow is based on Groovy. The choice between one of the other is really down to individual preference.\n\nAnother important aspect of these projects are the workflows and modules provided by the community:\n\nnf-core: a community project where scientists contribute ready-to-use, high-quality analysis pipelines. This means you don’t have to start from scratch if someone else has already created a workflow for a similar analysis. It’s all about making data analysis more accessible, standardized, and reproducible across the globe​​​​.\nSnakemake workflow catalog: a searcheable catalog of workflows developed by the community, with instructions and details on how to use them. Although there is some standardisation of these pipelines, they are not as well curated as the ones from nf-core.\n\nThese materials will focus on Nextflow, due to the standarised and ready-to-use pipelines available through nf-core.\n\n\n\n\n\n\nNoteHow do I install Nextflow and Snakemake?\n\n\n\nYou can install both of these packages using Mamba:\nmamba create -n nextflow bioconda::nextflow==25.10.3.10983\nmamba create -n snakemake bioconda::snakemake==9.15.0\nYou may want to check the latest versions available (here and here), which may be different from the ones in the command above.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#nextflow-command-line-interface",
    "href": "materials/03-nfcore.html#nextflow-command-line-interface",
    "title": "5  Automated Workflows",
    "section": "5.3 Nextflow command line interface",
    "text": "5.3 Nextflow command line interface\nNextflow has an array of subcommands for the command line interface to manage and execute pipelines. To see all options you can simply run nextflow -h and a list of available top-level options will appear in your terminal’. here we highlight the three we will be using:\n\nnextflow run [options] [pipeline]: will execute a nextflow pipeline\nnextflow log: will print the execution history and log information of a pipeline\nnextflow clean: will clean up cache and work directories.\n\nThe command nextflow run has some useful options:\n\n-profile: defines which configuration profile(s) to use.\n-resume: restarts the pipeline where it last stopped (using cached results).\n-work-dir: defines the directory where intermediate result files (cache) are stored.\n\nWe detail each of these below.\n\n5.3.1 Configuration profile\nThere are several ways to configure how our Nextflow workflow runs. All nf-core workflows come with some default profiles that we can choose from:\n\nsingularity uses Singularity images for software management. This is the recommended (including on HPC systems).\ndocker uses Docker images for software management.\nmamba uses Mamba for software management. This is not recommended as it is known to be slow and buggy.\n\nMore details about these in the nf-core documentation.\nTo use one of these profiles we use the option -profile followed by the name of the configuration profile we wish to use. For example, -profile singularity will use Singularity to manage and run the software.\nSometimes you may want to use custom profiles, or the pipeline you are using is not from the nf-core community. In that case, you can define your own profile. The easiest may be to look at one of the nf-core configuration files and set your own based on that. For example, to set a profile for Singularity, we create a file with the following:\nprofiles {\n  singularity {\n    singularity.enabled    = true\n  }\n}\nLet’s say we saved this file as nextflow.config. We can then use this profile by running our pipeline with the options -profile singularity -c nextflow.config. You can also specify more than one configuration file in your command as -c myconfig1 -c myconfig2. If you provide multiple config files, they will be merged so that the settings in the first override the same settings appearing in the second, and so on.\n\n\n\n\n\n\nNoteWorkflow configuration\n\n\n\nUnderstanding the config file of a nextflow pipeline can be slightly daunting at first, especially if you start with a nf-core configuration. For example, if you look at the default config for the nf-core/rnaseq workflow, you can see how many parameters it includes.\nThe good news is that normally users won’t need to modify most of these, instead you can just modify the parameters of interest and use -c your_config_profile.config when launching your pipeline.\nYou can learn more about configuration in the HPC and advanced configuration chapters.\n\n\n\n\n5.3.2 Cache directory\nWhen a Nextflow pipeline runs it creates (by default) a work directory when you execute the pipeline for the first time. The work directory stores a variety of intermediate files used during the pipeline run, called a cache. The storage of these intermediate files is very important, as it allows the pipeline to resume from a previous state, in case it ran with errors and failed half-way through (more on this below).\nEach task from the pipeline (e.g. a bash command can be considered a task) will have a unique directory name within work. When a task is created, Nextflow stages the task input files, script, and other helper files into the task directory. The task writes any output files to this directory during its execution, and Nextflow uses these output files for downstream tasks and/or publishing. Publishing is when the output of a task is being saved to the output directory specified by the user.\nThe -work-dir option can be used to change the name of the cache directory from the default work. This default directory name is fine (and most people just use that), but you may sometimes want to define a different one. For example, if you coincidentally already have a directory called “work” in your project, or if you want to use a separate storage partition to save the intermediate files.\nRegardless, it is important to remember that your final results are not stored in the work directory. They are saved to the output directory you define when you run the pipeline. Therefore, after successfully finishing your pipeline you can safely remove the work directory. This is important to save disk space and you should make sure to do it regularly.\n\n\n5.3.3 Checkpoint-and-resume\nBecause Nextflow is keeping track of all the intermediate files it generates, it can re-run the pipeline from a previous step, if it failed half-way through. This is an extremely useful feature of workflow management systems and it can save a lot of compute time, in case a pipeline failed.\nAll you have to do is use the option -resume when launching the pipeline and it will always resume where it left off. Note that, if you remove the work cache directory detailed above, then the pipeline will have to start from the beginning, as it doesn’t have any intermediate files saved to resume from.\nMore information about this feature can be found in the Nextflow documentation.\n\n\n5.3.4 Samplesheet\nMost nf-core pipelines use a CSV file as their main input file. These CSV file is often referred to as the “sampleshet”, as it contains information about each sample to be processed.\nAlthough there is no universal format for this CSV file, most pipelines accept at least 3 columns:\n\nsample: a name for the sample, which can be anything of our choice.\nfastq_1: the path to the respective read 1 FASTQ file.\nfastq_2: the path to the respective read 2 FASTQ file. This value is optional and, if missing it is assumed the sample is from single-end sequencing.\n\nThese are only the most basic columns, however many other columns are often accepted, depending on the specific pipeline being used. The details for the input CSV samplesheet are usually given in the “Usage” tab of the documentation. We will see some practical examples in Exercise 1.\nAs FASTQ files are often named using very long identifiers, it’s a good idea to use some command line tricks to save typing and avoid typos. For example, we can create a first version of our file by listing all read 1 files and saving it into a file:\nls reads/*_1.downsampled.fastq.gz &gt; samplesheet.csv\nThis will give us a good start to create the samplesheet. We can then open this file in a spreadsheet software such as Excel and create the remaining columns. We can copy-paste the file paths and use the “find and replace” feature to replace “_1” with “_2”. This way we save a lot of time of typing but also reduce the risk of having typos in our file paths.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#demo-nf-core-pipeline",
    "href": "materials/03-nfcore.html#demo-nf-core-pipeline",
    "title": "5  Automated Workflows",
    "section": "5.4 Demo nf-core pipeline",
    "text": "5.4 Demo nf-core pipeline\nTo demonstrate the use of standard nf-core pipelines, we will use the aptly named nf-core/demo pipeline. This workflow takes a set of FASTQ files as input, runs them through a simple QC step and outputs processed files as well as a MultiQC quality report.\n\n\n\nnf-core/demo workflow diagram by Christopher Hakkaart, licensed under MIT\n\n\nWe will run this workflow on a set of files found in the demo directory. We will start by activating a Mamba environment where we installed nextflow:\nmamba activate nextflow\nLooking at the pipeline documentation, we are given an example of the samplesheet CSV file. This is how the samplesheet looks like for our samples:\nsample,fastq_1,fastq_2\ndrug_rep2,reads/SRR7657872_1.downsampled.fastq.gz,reads/SRR7657872_2.downsampled.fastq.gz\ndrug_rep1,reads/SRR7657874_1.downsampled.fastq.gz,reads/SRR7657874_2.downsampled.fastq.gz\ncontrol_rep2,reads/SRR7657876_1.downsampled.fastq.gz,reads/SRR7657876_2.downsampled.fastq.gz\ncontrol_rep1,reads/SRR7657877_1.downsampled.fastq.gz,reads/SRR7657877_2.downsampled.fastq.gz\nWe have named our samples using informative names of our choice, and indicate the path to the respective FASTQ input files. We can then run our workflow as follows (this command is available from scripts/02-run_nfcore_demo.sh):\nnextflow run nf-core/demo \\\n  -profile \"singularity\" -revision \"1.1.0\" \\\n  --input \"samplesheet.csv\" \\\n  --outdir \"results/qc\" \\\n  --fasta \"genome/Mus_musculus.GRCm38.dna_sm.chr14.fa.gz\"\nIn this case we used the following generic options:\n\n-profile \"singularity\" indicates we want to use Singularity to manage the software. Nextflow will automatically download containers for each step of the pipeline.\n-revision \"1.1.0\" means we are running version 1.1.0 of the pipeline. It’s a good idea to define the specific version of the pipeline you run, so you can reproduce the results in the future, in case the pipeline changes. You can see the latest versions available from the workflow documentation page.\n\nWe then have workflow-specific options (all listed in the documentation):\n\n--input is the samplesheet CSV for this pipeline, which we prepared beforehand using a spreadsheet program such as Excel.\n--outdir is the name of the output directory for our results.\n--fasta is the reference genome to be used by the pipeline.\n\nWhen the pipeline starts running, we are given information about its progress, for example:\nexecutor &gt;  local (6)\n[d1/4efe7b] NFCORE_DEMO:DEMO:FASTQC (control_rep2)  | 4 of 4 ✔\n[4b/caf73e] NFCORE_DEMO:DEMO:SEQTK_TRIM (drug_rep2) | 1 of 4\n[-        ] NFCORE_DEMO:DEMO:MULTIQC                -\nYou will also notice that a new directory called work is created. As mentioned above, this is the cache directory, which stores intermediate files and allows the workflow to resume if it fails half-way through (using the -resume option).\nOnce the pipeline completes (hopefully successfully), we are given a message:\n-[nf-core/demo] Pipeline completed successfully-\nCompleted at: 11-Sep-2024 09:26:05\nDuration    : 11m 59s\nCPU hours   : 0.3\nSucceeded   : 9\n\n\n\n\n\n\nNoteReference genomes\n\n\n\nMany nf-core pipelines allow you to specific the name of an organism (using the --genome option) and will automatically download the reference genome and annotation files for you. However, many of these workflows rely on iGenomes, which is not always up-to-date. Therefore, the use of this option is discouraged, as you may miss the latest annotations or use a version of the genome that is incompatible with the rest of your analysis.\nInstead, you can download the latest version of the genome and annotations for your species, from sources such as ENSEMBL (vertebrates and non-vertebrates), GENCODE or UCSC.\nMost pipelines then have individual options to use these files as input: --fasta (for the reference genome) and --gff/--gtf (for the transcript annotation).\n\n\n\n5.4.1 Cleaning up\nIf you are happy with the results, you can clean the work cache directory to save space. Before actually removing anything, you can see what the clean command would do using the -dry-run (or -n) option:\nnextflow clean -n\nThis will inform you of what the command would remove. If you’re happy with this, you can go ahead and issue to command to -force (or -f) the removal:\nnextflow clean -f\nThe clean command has several options allowing you finer control over what gets deleted, for example the -before and -after options allow you to clean up cached files before or after the specified date/time.\nWhile nextflow clean works well, by default it still leaves behind some files. Usually these don’t occupy much space, but if you want to completely remove the cached files and hidden log files, you can do this manually:\nrm -r .nextflow* work",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#troubleshooting",
    "href": "materials/03-nfcore.html#troubleshooting",
    "title": "5  Automated Workflows",
    "section": "5.5 Troubleshooting",
    "text": "5.5 Troubleshooting\nInevitably workflows may fail, which could be due to several reasons. For example, an error in our command, a mis-formatted samplesheet, missing input files or sometimes even a bug in the pipeline.\nWhen an error occurs, the nextflow command terminates and an error message is printed on the screen (usually in bright red!). The error messages can be quite long and feel difficult to interpret, but often only a small part of the message is relevant, so read it carefully to see if you can spot what the problem is.\nFor example, we previously got the following error when running the nf-core/demo pipeline. Can you see what the problem was?\n\n-[nf-core/demo] Pipeline completed with errors-\n\nERROR ~ Error executing process &gt; 'NFCORE_DEMO:DEMO:FASTP (drug_rep1)'\n\nCaused by:\n  Process requirement exceeds available memory -- req: 36 GB; avail: 23.5 GB\n\nCommand executed:\n\n  [ ! -f  drug_rep1_1.fastq.gz ] && ln -sf SRR7657874_1.downsampled.fastq.gz drug_rep1_1.fastq.gz\n  [ ! -f  drug_rep1_2.fastq.gz ] && ln -sf SRR7657874_2.downsampled.fastq.gz drug_rep1_2.fastq.gz\n  fastp \\\n      --in1 drug_rep1_1.fastq.gz \\\n      --in2 drug_rep1_2.fastq.gz \\\n      --out1 drug_rep1_1.fastp.fastq.gz \\\n      --out2 drug_rep1_2.fastp.fastq.gz \\\n      --json drug_rep1.fastp.json \\\n      --html drug_rep1.fastp.html \\\n       \\\n       \\\n       \\\n      --thread 6 \\\n      --detect_adapter_for_pe \\\n       \\\n      2&gt; &gt;(tee drug_rep1.fastp.log &gt;&2)\n  \n  cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n  \"NFCORE_DEMO:DEMO:FASTP\":\n      fastp: $(fastp --version 2&gt;&1 | sed -e \"s/fastp //g\")\n  END_VERSIONS\n\n\n\n\n\nClick here for the answer\n\nAlthough this is a long message, the cause of the error itself is at the top where we are told “Process requirement exceeds available memory – req: 36 GB; avail: 23.5 GB”.\nThis means a step of the pipeline must have requested 36GB by default, but we only had 23.5GB on the computer used to run it. In this case, we could have restricted the memory usage with a custom configuration file, which we will discuss in the next section.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#exercises",
    "href": "materials/03-nfcore.html#exercises",
    "title": "5  Automated Workflows",
    "section": "5.6 Exercises",
    "text": "5.6 Exercises\nIn these exercises, you will explore one (or more, if you have time and interest) of the pipelines from the nf-core community, tailored to different areas of genomic analysis. Start with the version of the exercise that aligns best with your data and interests.\nThere are two tasks for you to complete: (1) preparing an input samplesheet for the pipeline and (2) writing the nextflow command to launch the pipeline.\nNote that all of these datasets are downsampled to be small, so they run quickly. They do not represent best practices in experimental design.\n\n\n\n\n\n\nExerciseExercise 1 - Preparing samplesheet\n\n\n\n\n\n\nMost nf-core pipelines require a samplesheet as input, which is essentially a CSV file detailing where to find the sequencing files for each sample. The specific format for the samplesheet is workflow-specific, but always detailed in the respective documentation. Your goal in this exercise is to find what the required format for the samplesheet is and create the CSV file to use in the next exercise.\nYou can create the CSV using a spreadsheet software (such as Excel or LibreOffice Calc).\nAs a bonus, you can try to create the samplesheet in a less manual way using the provided metadata files. You could do this using command line tools (such as awk or perl) or even using R or Python.\n\nRNA-seqChIP-seqVirus - IlluminaVirus - ONT\n\n\nTranscriptome data processing using nf-core/rnaseq. Go into the rnaseq directory for this version of the exercise.\n\nSamplesheet documentation at https://nf-co.re/rnaseq/3.22.2/docs/usage/.\nInput FASTQ files in reads/.\nMetadata for each sample is provided in the sample_info.tsv file, which gives you the name of each sample and their respective FASTQ file prefix name.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nOur metadata file contains information about each sample:\ncat sample_info.tsv\nfastq       replicate  status      timepoint  name\nSRR7657874  1          Infected    d33        inf33_rep1\nSRR7657872  3          Infected    d33        inf33_rep3\nSRR7657877  1          Uninfected  d11        un11_rep1\nSRR7657876  2          Uninfected  d11        un11_rep2\nIf we look inside our reads directory, we can see the following files:\nls reads\nSRR7657872_1.downsampled.fastq.gz  SRR7657876_1.downsampled.fastq.gz\nSRR7657872_2.downsampled.fastq.gz  SRR7657876_2.downsampled.fastq.gz\nSRR7657874_1.downsampled.fastq.gz  SRR7657877_1.downsampled.fastq.gz\nSRR7657874_2.downsampled.fastq.gz  SRR7657877_2.downsampled.fastq.gz\nFollowing the documentation for the pipeline, we could have created our samplesheet in Excel like this (note: the filenames are truncated):\n\n\n\n\n\nA\n\n\nB\n\n\nC\n\n\nD\n\n\n\n\n1\n\n\nsample\n\n\nfastq_1\n\n\nfastq_2\n\n\nstrandedness\n\n\n\n\n2\n\n\ninf33_rep1\n\n\nreads/SRR7657874_1.down …\n\n\nreads/SRR7657874_2.down …\n\n\nauto\n\n\n\n\n3\n\n\ninf33_rep3\n\n\nreads/SRR7657872_1.down …\n\n\nreads/SRR7657872_2.down …\n\n\nauto\n\n\n\n\n4\n\n\nun11_rep1\n\n\nreads/SRR7657877_1.down …\n\n\nreads/SRR7657877_2.down …\n\n\nauto\n\n\n\n\n5\n\n\nun11_rep2\n\n\nreads/SRR7657876_1.down …\n\n\nreads/SRR7657876_2.down …\n\n\nauto\n\n\n\nHowever, for many samples, it can be tedious and error-prone to do this kind of task manually. We can see that the FASTQ file names are essentially the prefix given in the metadata file, along with suffixes “_1.downsampled.fastq.gz” and “_2.downsampled.fastq.gz” for read 1 and read 2, respectively.\nTherefore, we can create the samplesheet based on our metadata TSV file using some programming tools. Here’s 4 different ways of doing it:\n\nPythonRPerlAWK\n\n\nimport pandas as pd\n\n# read metadata table, ensuring NA values are correctly imported\nmeta = pd.read_csv(\"sample_info.tsv\", sep=\"\\t\", na_values=\"\")\n\n# create data frame for output samplesheet\nout = pd.DataFrame({\n    'sample': meta['name'],\n    'fastq_1': \"reads/\" + meta['fastq'] + \"_1.downsampled.fastq.gz\",\n    'fastq_2': \"reads/\" + meta['fastq'] + \"_2.downsampled.fastq.gz\",\n    'strandedness': \"auto\"\n})\n\n# save the samplesheet ensuring NA values are written as empty cells\nout.to_csv(\"samplesheet.csv\", index=False, quoting=3, na_rep=\"\")\n\n\n# read metadata table, ensuring NA values are correctly imported\nmeta &lt;- read.table(\"sample_info.tsv\", header = TRUE, \n                   sep = \"\\t\", na.strings = \"\")\n\n# create data frame for output samplesheet\nout &lt;- data.frame(sample = meta$name, \n                  fastq_1 = paste0(\"reads/\", meta$fastq, \"_1.downsampled.fastq.gz\"), \n                  fastq_2 = paste0(\"reads/\", meta$fastq, \"_2.downsampled.fastq.gz\"),\n                  strandedness = \"auto\")\n\n# save the samplesheet ensuring NA values are written as empty cells\nwrite.csv(out, \"samplesheet.csv\", row.names = FALSE, quote = FALSE, na = \"\")\n\n\n# create new file with the required column names\necho \"sample,fastq_1,fastq_2,strandedness\" &gt; samplesheet.csv\n\n# generate file names from metadata\ntail -n +2 sample_info.tsv | perl -ne 'chomp;\n@a=split/\\t/;\nprint \"$a[4],reads/$a[0]\\_1.downsampled.fastq.gz,reads/$a[0]\\_2.downsampled.fastq.gz,auto\\n\"' &gt;&gt; samplesheet.csv\n\n\n# create new file with the required column names\necho \"sample,fastq_1,fastq_2,strandedness\" &gt; samplesheet.csv\n\n# generate file names from metadata\ntail -n +2 sample_info.tsv | awk 'BEGIN { FS=\"\\t\"; OFS=\",\" }\n{\n  print $5, \"reads/\" $1 \"_1.downsampled.fastq.gz\", \"reads/\" $1 \"_2.downsampled.fastq.gz\", \"auto\"\n}' &gt;&gt; samplesheet.csv\n\n\n\nNote, if all of the coding suggestions above seem unclear, we reiterate that you can create the samplesheet by hand in a standard spreadsheet software. At the end, our samplesheet should look like this:\nsample,fastq_1,fastq_2,strandedness\ninf33_rep1,reads/SRR7657874_1.downsampled.fastq.gz,reads/SRR7657874_2.downsampled.fastq.gz,auto\ninf33_rep3,reads/SRR7657872_1.downsampled.fastq.gz,reads/SRR7657872_2.downsampled.fastq.gz,auto\nun11_rep1,reads/SRR7657877_1.downsampled.fastq.gz,reads/SRR7657877_2.downsampled.fastq.gz,auto\nun11_rep2,reads/SRR7657876_1.downsampled.fastq.gz,reads/SRR7657876_2.downsampled.fastq.gz,auto\n\n\n\n\n\n\nTranscriptome data processing using nf-core/chipseq. Go into the chipseq directory for this version of the exercise.\n\nSamplesheet documentation at nf-co.re/chipseq/2.1.0/docs/usage/.\nInput FASTQ files in reads/.\nMetadata for each sample is provided in the sample_info.tsv file, which gives you the name of each sample and their respective FASTQ file prefix name.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nOur metadata file contains information about each sample:\ncat sample_info.tsv\nname            fastq       antibody  input_control\nbrd4_veh_rep1   SRR1193526  BRD4      mcf7_input_veh\nbrd4_veh_rep2   SRR1193527  BRD4      mcf7_input_veh\nbrd4_e2_rep1    SRR1193529  BRD4      mcf7_input_e2\nbrd4_e2_rep2    SRR1193530  BRD4      mcf7_input_e2\nmcf7_input_veh  SRR1193562            \nmcf7_input_e2   SRR1193563\nIf we look inside our reads directory, we can see the following files:\nls reads\nSRR1193526.fastq.gz  SRR1193530.fastq.gz\nSRR1193527.fastq.gz  SRR1193562.fastq.gz\nSRR1193529.fastq.gz  SRR1193563.fastq.gz\nFollowing the documentation for the pipeline, we could have created our samplesheet in Excel like this:\n\n\n\n\n\nA\n\n\nB\n\n\nC\n\n\nD\n\n\nE\n\n\nF\n\n\nG\n\n\n\n\n1\n\n\nsample\n\n\nfastq_1\n\n\nfastq_2\n\n\nreplicate\n\n\nantibody\n\n\ncontrol\n\n\ncontrol_replicate\n\n\n\n\n2\n\n\nbrd4_veh\n\n\nreads/SRR1193526.fastq.gz\n\n\n\n\n1\n\n\nBRD4\n\n\nmcf7_input_veh\n\n\n1\n\n\n\n\n3\n\n\nbrd4_veh\n\n\nreads/SRR1193527.fastq.gz\n\n\n\n\n2\n\n\nBRD4\n\n\nmcf7_input_veh\n\n\n1\n\n\n\n\n4\n\n\nbrd4_e2\n\n\nreads/SRR1193529.fastq.gz\n\n\n\n\n1\n\n\nBRD4\n\n\nmcf7_input_e2\n\n\n1\n\n\n\n\n5\n\n\nbrd4_e2\n\n\nreads/SRR1193530.fastq.gz\n\n\n\n\n2\n\n\nBRD4\n\n\nmcf7_input_e2\n\n\n1\n\n\n\n\n6\n\n\nmcf7_input_veh\n\n\nreads/SRR1193562.fastq.gz\n\n\n\n\n1\n\n\n\n\n\n\n\n\n\n\n7\n\n\nmcf7_input_e2\n\n\nreads/SRR1193563.fastq.gz\n\n\n\n\n1\n\n\n\n\n\n\n\n\n\nHowever, for many samples, it can be tedious and error-prone to do this kind of task manually. We can see that the FASTQ file names are essentially the prefix given in the metadata file, along with the suffix “.fastq.gz” (these are single-end sequencing data, so there’s only one file per sample).\nTherefore, we can create the samplesheet based on our metadata TSV file using some programming tools. Here’s 4 different ways of doing it:\n\nPythonRPerlAWK\n\n\nimport pandas as pd\n\n# read metadata table, ensuring NA values are correctly imported\nmeta = pd.read_csv(\"sample_info.tsv\", sep=\"\\t\", na_values=\"\")\n\n# create data frame for output samplesheet\n# \"Int64\" is used to ensure replicate columns output as integers\nout = pd.DataFrame({\n    \"sample\": meta[\"name\"],\n    \"fastq_1\": \"reads/\" + meta[\"fastq\"] + \".fastq.gz\",\n    \"fastq_2\": pd.NA,\n    \"replicate\": meta[\"rep\"].astype(\"Int64\"),\n    \"antibody\": meta[\"antibody\"],\n    \"control\": meta[\"input_control\"],\n    \"control_replicate\": meta[\"input_rep\"].astype(\"Int64\")\n})\n\n# save the samplesheet ensuring NA values are written as empty cells\nout.to_csv(\"samplesheet.csv\", index=False, quoting=3, na_rep=\"\")\n\n\n# read metadata table, ensuring NA values are correctly imported\nmeta &lt;- read.table(\"sample_info.tsv\", header = TRUE, \n                   sep = \"\\t\", na.strings = \"\")\n\n# create data frame for output samplesheet\nout &lt;- data.frame(sample = meta$name, \n                  fastq_1 = paste0(\"reads/\", meta$fastq, \".fastq.gz\"), \n                  fastq_2 = NA,\n                  replicate = meta$rep,\n                  antibody = meta$antibody,\n                  control = meta$input_control, \n                  control_replicate = meta$input_rep)\n\n# save the samplesheet ensuring NA values are written as empty cells\nwrite.csv(out, \"samplesheet.csv\", row.names = FALSE, quote = FALSE, na = \"\")\n\n\n# create new file with the required column names\necho \"sample,fastq_1,fastq_2,replicate,antibody,control,control_replicate\" &gt; samplesheet.csv\n\n# generate file names from metadata\ntail -n +2 sample_info.tsv | perl -ne 'chomp;\n@a=split/\\t/;\nprint \"$a[0],reads/$a[2].fastq.gz,,$a[1],$a[3],$a[4],$a[5]\\n\"' &gt;&gt; samplesheet.csv\n\n\n# create new file with the required column names\necho \"sample,fastq_1,fastq_2,replicate,antibody,control,control_replicate\" &gt; samplesheet.csv\n\n# generate file names from metadata\ntail -n +2 sample_info.tsv | awk 'BEGIN { FS=\"\\t\"; OFS=\",\" }\n{\n  print $1, \"reads/\" $3 \".fastq.gz\", \"\", $2, $4, $5, $6\n}' &gt;&gt; samplesheet.csv\n\n\n\nNote, if all of the coding suggestions above seem unclear, we reiterate that you can create the samplesheet by hand in a standard spreadsheet software. At the end, our samplesheet should look like this:\nsample,fastq_1,fastq_2,replicate,antibody,control,control_replicate\nbrd4_veh,reads/SRR1193526.fastq.gz,,1,BRD4,mcf7_input_veh,1\nbrd4_veh,reads/SRR1193527.fastq.gz,,2,BRD4,mcf7_input_veh,1\nbrd4_e2,reads/SRR1193529.fastq.gz,,1,BRD4,mcf7_input_e2,1\nbrd4_e2,reads/SRR1193530.fastq.gz,,2,BRD4,mcf7_input_e2,1\nmcf7_input_veh,reads/SRR1193562.fastq.gz,,1,,,\nmcf7_input_e2,reads/SRR1193563.fastq.gz,,1,,,\n\n\n\n\n\n\nTranscriptome data processing using nf-core/viralrecon. Go into the virus_illumina directory for this version of the exercise.\n\nSamplesheet documentation at nf-co.re/viralrecon/3.0.0/docs/usage/.\nInput FASTQ files in reads/.\nMetadata for each sample is provided in the sample_info.tsv file, which gives you the name of each sample and their respective FASTQ file prefix name.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nOur metadata file contains information about each sample:\ncat sample_info.tsv\nname    fastq\nZA01    SRR17051908\nZA02    SRR17051923\nZA03    SRR17051916\nZA04    SRR17051953\nZA05    SRR17051951\nZA06    SRR17051935\nZA07    SRR17051932\nZA08    SRR17054503\nIf we look inside our reads directory, we can see the following files:\nls reads\nSRR17051908_1.fastq.gz  SRR17051923_1.fastq.gz  SRR17051935_1.fastq.gz  SRR17051953_1.fastq.gz\nSRR17051908_2.fastq.gz  SRR17051923_2.fastq.gz  SRR17051935_2.fastq.gz  SRR17051953_2.fastq.gz\nSRR17051916_1.fastq.gz  SRR17051932_1.fastq.gz  SRR17051951_1.fastq.gz  SRR17054503_1.fastq.gz\nSRR17051916_2.fastq.gz  SRR17051932_2.fastq.gz  SRR17051951_2.fastq.gz  SRR17054503_2.fastq.gz\nFollowing the documentation for the pipeline, we could have created our samplesheet in Excel like this:\n\n\n\n\n\nA\n\n\nB\n\n\nC\n\n\n\n\n1\n\n\nsample\n\n\nfastq_1\n\n\nfastq_2\n\n\n\n\n2\n\n\nZA01\n\n\nSRR17051908_1.fastq.gz\n\n\nSRR17051908_2.fastq.gz\n\n\n\n\n3\n\n\nZA02\n\n\nSRR17051923_1.fastq.gz\n\n\nSRR17051923_2.fastq.gz\n\n\n\n\n4\n\n\nZA03\n\n\nSRR17051916_1.fastq.gz\n\n\nSRR17051916_2.fastq.gz\n\n\n\n\n5\n\n\nZA04\n\n\nSRR17051953_1.fastq.gz\n\n\nSRR17051953_2.fastq.gz\n\n\n\n\n6\n\n\nZA05\n\n\nSRR17051951_1.fastq.gz\n\n\nSRR17051951_2.fastq.gz\n\n\n\n\n7\n\n\nZA06\n\n\nSRR17051935_1.fastq.gz\n\n\nSRR17051935_2.fastq.gz\n\n\n\n\n8\n\n\nZA07\n\n\nSRR17051932_1.fastq.gz\n\n\nSRR17051932_2.fastq.gz\n\n\n\n\n9\n\n\nZA08\n\n\nSRR17054503_1.fastq.gz\n\n\nSRR17054503_2.fastq.gz\n\n\n\nHowever, for many samples, it can be tedious and error-prone to do this kind of task manually. We can see that the FASTQ file names are essentially the prefix given in the metadata file, along with the suffix “_1.fastq.gz” and “_2.fastq.gz” for read 1 and read 2, respectively.\nTherefore, we can create the samplesheet based on our metadata TSV file using some programming tools. Here’s 4 different ways of doing it:\n\nPythonRPerlAWK\n\n\nimport pandas as pd\n\n# read metadata table, ensuring NA values are correctly imported\nmeta = pd.read_csv(\"sample_info.tsv\", sep=\"\\t\", na_values=\"\")\n\n# create data frame for output samplesheet\nout = pd.DataFrame({\n    'sample': meta['name'],\n    'fastq_1': \"reads/\" + meta['fastq'] + \"_1.fastq.gz\",\n    'fastq_2': \"reads/\" + meta['fastq'] + \"_2.fastq.gz\"\n})\n\n# save the samplesheet ensuring NA values are written as empty cells\nout.to_csv(\"samplesheet.csv\", index=False, quoting=3, na_rep=\"\")\n\n\n# read metadata table, ensuring NA values are correctly imported\nmeta &lt;- read.table(\"sample_info.tsv\", header = TRUE, \n                   sep = \"\\t\", na.strings = \"\")\n\n# create data frame for output samplesheet\nout &lt;- data.frame(sample = meta$name, \n                  fastq_1 = paste0(\"reads/\", meta$fastq, \"_1.fastq.gz\"), \n                  fastq_2 = paste0(\"reads/\", meta$fastq, \"_2.fastq.gz\"))\n\n# save the samplesheet ensuring NA values are written as empty cells\nwrite.csv(out, \"samplesheet.csv\", row.names = FALSE, quote = FALSE, na = \"\")\n\n\n# create new file with the required column names\necho \"sample,fastq_1,fastq_2\" &gt; samplesheet.csv\n\n# generate file names from metadata\ntail -n +2 sample_info.tsv | perl -ne 'chomp;\n@a=split/\\t/;\nprint \"$a[0],reads/$a[1]\\_1.fastq.gz,reads/$a[1]\\_2.fastq.gz\\n\"' &gt;&gt; samplesheet.csv\n\n\n# create new file with the required column names\necho \"sample,fastq_1,fastq_2\" &gt; samplesheet.csv\n\n# generate file names from metadata\ntail -n +2 sample_info.tsv | awk 'BEGIN { FS=\"\\t\"; OFS=\",\" }\n{\n  print $1, \"reads/\" $2 \"_1.fastq.gz\", \"reads/\" $2 \"_2.fastq.gz\"\n}' &gt;&gt; samplesheet.csv\n\n\n\nNote, if all of the coding suggestions above seem unclear, we reiterate that you can create the samplesheet by hand in a standard spreadsheet software. At the end, our samplesheet should look like this:\nsample,fastq_1,fastq_2\nZA01,reads/SRR17051908_1.fastq.gz,reads/SRR17051908_2.fastq.gz\nZA02,reads/SRR17051923_1.fastq.gz,reads/SRR17051923_2.fastq.gz\nZA03,reads/SRR17051916_1.fastq.gz,reads/SRR17051916_2.fastq.gz\nZA04,reads/SRR17051953_1.fastq.gz,reads/SRR17051953_2.fastq.gz\nZA05,reads/SRR17051951_1.fastq.gz,reads/SRR17051951_2.fastq.gz\nZA06,reads/SRR17051935_1.fastq.gz,reads/SRR17051935_2.fastq.gz\nZA07,reads/SRR17051932_1.fastq.gz,reads/SRR17051932_2.fastq.gz\nZA08,reads/SRR17054503_1.fastq.gz,reads/SRR17054503_2.fastq.gz\n\n\n\n\n\n\nTranscriptome data processing using nf-core/viralrecon. Go into the virus_ont directory for this version of the exercise.\n\nSamplesheet documentation at nf-co.re/viralrecon/3.0.0/docs/usage/.\nInput barcode directories with FASTQ files in fastq_pass/. This is the directory that is created by standard ONT basecalling software such as Guppy or Dorado.\nMetadata for each sample is provided in the sample_info.tsv file, which gives you the name of each sample and their respective barcode folder.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nOur metadata file contains information about each sample:\ncat sample_info.tsv\nname    ont_barcode\nCH01    1\nCH02    2\nCH03    3\nCH04    4\nCH05    5\nCH26    26\nCH27    27\nCH28    28\nCH29    29\nCH30    30\nIf we look inside our reads directory, we can see the following directories:\nls fastq_pass\nbarcode01  barcode03  barcode05  barcode27  barcode29\nbarcode02  barcode04  barcode26  barcode28  barcode30\nFollowing the documentation for the pipeline, we could have created our samplesheet in Excel like this:\n\n\n\n\n\nA\n\n\nB\n\n\n\n\n1\n\n\nsample\n\n\nbarcode\n\n\n\n\n2\n\n\nCH01\n\n\n1\n\n\n\n\n3\n\n\nCH02\n\n\n2\n\n\n\n\n4\n\n\nCH03\n\n\n3\n\n\n\n\n5\n\n\nCH04\n\n\n4\n\n\n\n\n6\n\n\nCH05\n\n\n5\n\n\n\n\n7\n\n\nCH26\n\n\n26\n\n\n\n\n8\n\n\nCH27\n\n\n27\n\n\n\n\n9\n\n\nCH28\n\n\n28\n\n\n\n\n10\n\n\nCH29\n\n\n29\n\n\n\n\n11\n\n\nCH30\n\n\n30\n\n\n\nCompared to the other workflows, this one is a relatively simple format, so given our metadata table, it would be easy to create the required samplesheet in Excel. However, for completeness we also show how to do this using the command line to replace tabs for commas using the sed command:\n# create new file with the required column names\necho \"sample,barcode\" &gt; samplesheet.csv\n\n# generate file names from metadata\ntail -n +2 sample_info.tsv | sed 's/\\t/,/' &gt;&gt; samplesheet.csv\nAt the end, our samplesheet should look like this:\nsample,barcode\nCH01,1\nCH02,2\nCH03,3\nCH04,4\nCH05,5\nCH26,26\nCH27,27\nCH28,28\nCH29,29\nCH30,30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseExercise 2 - Running the pipeline\n\n\n\n\n\n\nNow that you have a samplesheet, you should be ready to write the command to run the pipeline. Then, you can launch the pipeline and watch how Nextflow orchestrates the different steps of the analysis.\nGiven the complexity of some of these pipelines, we give a skeleton of a command to get you started in each case. However, a few things to remember to do in every case:\n\nAdd the -r option to specify the version of the workflow to be used (use the latest available on the respective documentation page).\nInclude the option to use singularity to manage the software.\nCheck the parameters being used in the respective documentation page.\n\nTo make your code editing easier, make sure to include your command in a shell script and then run it using bash. Also, make sure to activate the Nextflow mamba environment (mamba activate nextflow) before you run the script.\nNote: some of these pipelines take a long time to run. Once you initiate the pipeline and it seems to be running successfully, you can indicate to the trainers that you have completed the exercise.\n\nRNA-seqChIP-seqVirus - IlluminaVirus - ONT\n\n\nTranscriptome data processing using nf-core/rnaseq. Go into the rnaseq directory for this version of the exercise.\n\nParameter documentation at nf-co.re/rnaseq/3.22.2/parameters.\nUse the samplesheet created in the previous exercise as input.\nThe reference genome and annotation are in the genome/ directory.\n\nNote that the options to specify these input files require the full path to the file to be specified. In the code skeleton below we show the trick of using the $PWD environment variable to specify the path relative to the current working directory.\n\n\nWe provide a skeleton of what your command should look like in the script scripts/run_rnaseq.sh. Open that script in a text editor (for example, nano or vim) to fix the code and then run the script using bash.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nHere is the fixed nextflow command in our script:\nnextflow run nf-core/rnaseq \\\n  -r \"3.22.2\" \\\n  -profile \"singularity\" \\\n  --input \"samplesheet.csv\" \\\n  --outdir \"results/rnaseq\" \\\n  --gtf \"$PWD/genome/Mus_musculus.GRCm38.102.chr14.gtf.gz\" \\\n  --fasta \"$PWD/genome/Mus_musculus.GRCm38.dna_sm.chr14.fa.gz\"  \\\n  --igenomes_ignore \\\n  --skip_alignment\nWe then ran our script with bash scripts/run_rnaseq.sh. While running, we got the progress of the pipeline printed on the screen.\nAt the end of the pipeline we could see the results in the results/rnaseq folder. For example, we can open the file results/rnaseq/multiqc/star_salmon/multiqc_report.html to look at a quality control report for the pipeline.\n\n\n\n\n\n\nWarning\n\n\n\nIn the code above we’ve used the option --skip_alignment. In real-world analysis we would not recommend doing this by default, as the alignment provides many useful QC metrics for RNA-seq (e.g. gene body coverage, PCR duplication issues, etc.).\nSkipping the alignment does result in a much faster run, so it may be justified, for example, if you are processing a very large number of samples and need to save compute resources.\nIn this case, we used it to save time, as our purpose is to show how to set and run an nf-core pipeline, not to get precise results.\n\n\n\n\n\n\n\n\nChromatin immunoprecipitation sequencing analysis using nf-core/chipseq. Go into the chipseq directory for this version of the exercise.\n\nParameter documentation at nf-co.re/chipseq/2.1.0/parameters.\nUse the samplesheet created in the previous exercise as input.\nThe reference genome and annotation are in the genome/ directory.\n\nNote that the options to specify these input files require the full path to the file to be specified. In the code skeleton below we show the trick of using the $PWD environment variable to specify the path relative to the current working directory.\n\n\nWe provide a skeleton of what your command should look like in the script scripts/run_chipseq.sh. Open that script in a text editor (for example, nano or vim) to fix the code and then run the script using bash.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nHere is the fixed nextflow command in our script:\nnextflow run nf-core/chipseq \\\n  -r \"2.1.0\" \\\n  -profile \"singularity\" \\\n  --input \"samplesheet.csv\" \\\n  --outdir \"results/chipseq\" \\\n  --gtf \"$PWD/genome/GRCh38.109.chr21.gtf.gz\" \\\n  --fasta \"$PWD/genome/GRCh38.109.chr21.fasta.gz\" \\\n  --blacklist \"$PWD/genome/ENCFF356LFX_exclusion_lists.chr21.bed.gz\" \\\n  --read_length 100 \\\n  --skip_preseq\nWe then ran our script with bash scripts/run_chipseq.sh. While running, we got the progress of the pipeline printed on the screen.\nAt the end of the pipeline we could see the results in the results/chipseq folder. For example, we can open the file results/chipseq/multiqc/broadPeak/multiqc_report.html to look at a quality control report for the pipeline.\n\n\n\n\n\n\nAnalysis of viral genomes using nf-core/viralrecon. Go into the virus_illumina directory for this version of the exercise.\n\nParameter documentation at nf-co.re/viralrecon/3.0.0/parameters.\nUse the samplesheet created in the previous exercise as input.\nThe reference genome, gene annotation and primer locations are in the genome/ directory.\n\nNote that the options to specify these input files require the full path to the file to be specified. In the code skeleton below we show the trick of using the $PWD environment variable to specify the path relative to the current working directory.\n\n\nWe provide a skeleton of what your command should look like in the script scripts/run_viralrecon_illumina.sh. Open that script in a text editor (for example, nano or vim) to fix the code and then run the script using bash.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nHere is the fixed nextflow command in our script:\nnextflow run nf-core/viralrecon \\\n  -r \"3.0.0\" \\\n  -profile \"singularity\" \\\n  --input \"samplesheet.csv\" \\\n  --outdir \"results/viralrecon\" \\\n  --platform \"illumina\" \\\n  --protocol \"amplicon\" \\\n  --gff \"$PWD/genome/nCoV-2019.annotation.gff.gz\" \\\n  --fasta \"$PWD/genome/nCoV-2019.reference.fasta\" \\\n  --primer_bed \"$PWD/genome/nCoV-2019.V3.primer.bed\" \\\n  --skip_assembly --skip_freyja \\\n  --skip_pangolin --skip_nextclade\nWe then ran our script with bash scripts/run_viralrecon.sh. While running, we got the progress of the pipeline printed on the screen.\nAt the end of the pipeline we could see the results in the results/viralrecon folder. For example, we can open the file results/viralrecon/multiqc/multiqc_report.html to look at a quality control report for the pipeline.\n\n\n\n\n\n\nAnalysis of viral genomes using nf-core/viralrecon. Go into the virus_ont directory for this version of the exercise.\n\nParameter documentation at nf-co.re/viralrecon/3.0.0/parameters.\nUse the samplesheet created in the previous exercise as input.\nThe reference genome, gene annotation and primer locations are in the genome/ directory.\n\nNote that the options to specify these input files require the full path to the file to be specified. In the code skeleton below we show the trick of using the $PWD environment variable to specify the path relative to the current working directory.\n\n\nWe provide a skeleton of what your command should look like in the script scripts/run_viralrecon_ont.sh. Open that script in a text editor (for example, nano or vim) to fix the code and then run the script using bash.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nThe nanopore sub-workflow of the viralrecon pipeline is based on the ARTIC bioinformatics protocol and uses several of the tools from the accompanying artic software package.\nHere is the fixed nextflow command in our script:\nnextflow run nf-core/viralrecon \\\n  -r \"3.0.0\" \\\n  -profile \"singularity\" \\\n  --input \"samplesheet.csv\" \\\n  --fastq_dir \"fastq_pass/\" \\\n  --outdir \"results/viralrecon\" \\\n  --platform \"nanopore\" \\\n  --protocol \"amplicon\" \\\n  --genome \"MN908947.3\" \\\n  --primer_set \"artic\" \\\n  --primer_set_version \"3\" \\\n  --artic_minion_model \"r941_prom_sup_g5014\" \\\n  --skip_assembly --skip_freyja \\\n  --skip_pangolin --skip_nextclade\nWe then ran our script with bash scripts/run_viralrecon.sh. While running, we got the progress of the pipeline printed on the screen.\nAt the end of the pipeline we could see the results in the results/viralrecon folder. For example, we can open the file results/viralrecon/multiqc/medaka/multiqc_report.html to look at a quality control report for the pipeline.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#summary",
    "href": "materials/03-nfcore.html#summary",
    "title": "5  Automated Workflows",
    "section": "5.7 Summary",
    "text": "5.7 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nWfMS define, automate and monitor the execution of a series of tasks in a specific order. They improve efficiency, reduce errors, can be easily scaled (from a local computer to a HPC cluster) and increase reproducibility.\nPopular WfMS in bioinformatics include Nextflow and Snakemake. Both of these projects have associated community-maintained workflows, with excellent documentation for their use: nf-core and the snakemake workflow catalog.\nNextflow pipelines have configuration profiles available to indicate how software should be managed by the pipeline. For example the option -profile singularity uses Singularity images to deploy the software (other options include docker and conda).\nNextflow keeps track of the current status of the pipeline by storing intermediate files in a cache directory (by default called “work”). This enables the workflow to resume from a previous run, in case of failure.\nPipelines from the nf-core community commonly take as an input a CSV file detailing the input files for the workflow. This CSV file is commonly referred to as a samplesheet.\nNf-core pipelines have extensive documentation at nf-co.re/pipelines, allowing the user to configure many aspects of the run.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/03-nfcore.html#footnotes",
    "href": "materials/03-nfcore.html#footnotes",
    "title": "5  Automated Workflows",
    "section": "",
    "text": "To learn how to build your own pipelines, there are many tutorials available on training.nextflow.io, including how to build a simple RNA-Seq workflow. Snakemake also provides an excellent tutorial covering both basic and advanced features to build custom pipelines.↩︎",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automated Workflows</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow_hpc.html",
    "href": "materials/04-nextflow_hpc.html",
    "title": "6  Nextflow on HPC",
    "section": "",
    "text": "6.1 Nextflow HPC configuration\nTo run our workflows on an HPC, it is advisable to specify a configuration file tailored to your system. This file should include:\nAs briefly mentioned in the previous chapter, a config file is a set of attributes used by Nextflow when running a pipeline. By default, Nextflow will look for configuration files in predefined locations (see advanced configuration), but you can also specify a config file using the -c option.\nBelow is an example of a configuration file for an HPC that uses SLURM as the job scheduler. This example is based on the Cambridge University HPC, specifically the “cascade lake” nodes (docs).\nHere is an explanation of this configuration:\nProper executor configuration is crucial for running your jobs efficiently on the HPC, so make sure you spend some time configuring it correctly.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Nextflow on HPC</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow_hpc.html#nextflow-hpc-configuration",
    "href": "materials/04-nextflow_hpc.html#nextflow-hpc-configuration",
    "title": "6  Nextflow on HPC",
    "section": "",
    "text": "The job scheduler used to manage jobs (e.g. SLURM, LSF, PBS, HTCondor, etc.).\nJob submission details, such as your billing account (if applicable), the partition or queue to submit jobs to, and other relevant information.\nResource specifications like maximum RAM and CPUs available on the partitions/queues.\nSettings that control job submission, including the number of concurrent jobs, submission rate, and how often Nextflow checks job statuses.\n\n\n\n// See more info on process scope here: https://www.nextflow.io/docs/latest/config.html#scope-process\nprocess {\n    // Our job scheduling system or executor\n    // many executors supported (cloud and HPC): https://www.nextflow.io/docs/latest/executor.html\n    executor = 'slurm'\n    \n    // the queue or partition we want to use\n    queue = 'cclake'\n    \n    // Specify MAX parameters to avoid going over the limits\n    // these values should match the resources in the chosen queue/partition\n    resourceLimits = [\n      cpus: '56',\n      memory: '192.GB',\n      time: '36.h'\n    ]\n}\n\n// Limit nextflow submissions rates to a reasonable level to be kind to other users\n// See all options here: https://www.nextflow.io/docs/latest/config.html#scope-executor\nexecutor {\n    account             = 'YOUR-BILLING-ACCOUNT'\n    perCpuMemAllocation = '3410MB'\n    queueSize           = '200'\n    pollInterval        = '3 min'\n    queueStatInterval   = '5 min'\n    submitRateLimit     = '50sec'\n    exitReadTimeout     = '5 min'\n}\n\n// Options when using the singularity profile\nsingularity {\n    enabled = true\n    // useful if you are unsure about filesystem binding\n    autoMounts = true\n    // Allow extra time to pull out a container in case the servers are slow\n    pullTimeout = '1 h'  \n    // Specify a cache dir to re-use images that have already been downloaded\n    cacheDir = 'PATH/TO/nextflow-singularity-cache'\n}\n\n\nThe process directive defines:\n\nThe executor, which in this example is SLURM (the job scheduler). By default, this option would be “local”, meaning commands run on the current computer.\nThe queue, which corresponds to the SLURM --partition option, determining the type of node your jobs will run on.\n\nThe executor directive further configures the job scheduler:\n\naccount is the billing account (if relevant), equivalent to -A option in SLURM.\nperCpuMemAllocation submits jobs using --mem-per-cpu, relevant for the Cambridge HPC. This is optional and may vary by institution.\nqueueSize limits the number of simultaneous jobs in the queue. HPC admins may impose limits, so adjust this accordingly. Even with high limits, it’s advisable to limit simultaneous jobs to reduce the load on the job scheduler.\npollInterval, queueStatInterval, submitRateLimit and exitReadTimeout are settings that manage how often Nextflow checks job statuses and interacts with the scheduler. These settings help ensure that you use the scheduler efficiently and ethically. Rapid job submissions and frequent queue checks can overload the scheduler and might trigger warnings from HPC admins.\n\nThe singularity directive configures Singularity for running pipelines in an isolated software environment. This can be set up using the singularity scope.\n\nautoMounts = true automatically mounts the filesystem, which is helpful if you’re unfamiliar with filesystem bindings. On most HPC systems, admins handle this, so you may not need to worry about it.\ncacheDir ensures that previously downloaded images aren’t downloaded again. This is beneficial if you run the same pipeline multiple times or different pipelines that use the same software images. We recommend setting up a cache directory in a location accessible from the compute nodes.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Nextflow on HPC</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow_hpc.html#running-nextflow-on-a-hpc",
    "href": "materials/04-nextflow_hpc.html#running-nextflow-on-a-hpc",
    "title": "6  Nextflow on HPC",
    "section": "6.2 Running Nextflow on a HPC",
    "text": "6.2 Running Nextflow on a HPC\nWhen working on an HPC cluster, you typically interact with two types of nodes:\n\nThe head or login node, used for low-resource tasks such as navigating the filesystem, moving files, editing scripts, and submitting jobs to the scheduler.\nThe compute nodes, where computationally intensive tasks are executed, typically managed by the job scheduler.\n\nYou might wonder if it’s acceptable to run your Nextflow command directly on the HPC head/login node. Generally, this is perfectly fine because Nextflow itself doesn’t consume a lot of resources. The main Nextflow process handles interactions with the job scheduler (e.g. SLURM), checks job statuses in the queue, submits new jobs as needed, and logs progress information. Essentially, it automates the process of submitting and tracking jobs, so it isn’t computationally demanding.\nHowever, it’s important to ensure that your Nextflow process continues to run even if you log out of the HPC (which you’ll likely want to do, as workflows can take hours or even days to complete!). There are two primary ways to achieve this: running Nextflow as a background process or using a persistent terminal with a terminal multiplexer.\n\n6.2.1 Nextflow as a background process\nThe nextflow command has the -bg option, which allows you to run the process in the background. If you want to check on the progress of your Nextflow run, you can review the .nextflow.log file, which logs the workflow’s progress in a text format.\n\n\n6.2.2 Persistent terminal\nIf you prefer interactive output on the terminal, we recommend using a terminal multiplexer. A terminal multiplexer lets you open “virtual terminals” that continue running in the background, allowing processes to persist even if you close your window or disconnect from the HPC.\nTwo popular and widely available terminal multiplexers are screen and tmux. Both work similarly, and we’ll briefly demonstrate their usage below.\nThe first step is to start a session called “demo”:\n\nFor screen: screen -S demo (note the uppercase -S)\nFor tmux: tmux new -s demo\n\nThis opens a session, which essentially looks like your regular terminal. However, you can detach from this session, leaving it running in the background and come back to it later.\nAs an illustrative example, let’s run the following command, which counts to 600 every second:\nfor i in {1..600}; do echo $i; sleep 1; done\nThis command will run for 10 minutes. Imagine this was your Nextflow process, printing pipeline progress on the screen.\nIf you want to log out of the HPC and leave this task running, you can detach the session, returning to the main terminal:\n\nFor screen: press Ctrl + A then D\nFor tmux: press Ctrl + B then D\n\nFinally, log out from the HPC (e.g. using the exit command). Before logging out, it’s a good idea to note the node you’re on. One way to do this is with the hostname command.\nSuppose your login node was called login-09. You can log back into this specific node as follows:\nssh username@login-09.train.bio\nOnce back in your terminal, you can list any running sessions:\n\nscreen -ls\ntmux ls\n\nYou should see your demo session listed. To reattach to your session:\n\nscreen -r demo\ntmux attach -t demo\n\nYou’ll find your command still running in the background!",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Nextflow on HPC</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow_hpc.html#exercises",
    "href": "materials/04-nextflow_hpc.html#exercises",
    "title": "6  Nextflow on HPC",
    "section": "6.3 Exercises",
    "text": "6.3 Exercises\n\n\n\n\n\n\nExerciseExercise 1 - Nextflow HPC config\n\n\n\n\n\n\nWe have a training HPC available with the following characteristics:\n\nJob scheduler: SLURM\nMain queue/partition: normal\nBilling account: not used (can be omitted from config)\nQueue limits: 8 CPUs and 20GB of RAM\nJob duration: maximum 24 hours\nHigh-performance directory: /data/participant, shared by both the login and compute nodes\n\nFirst, login to the HPC with the command: ssh participant@train.hpc\nIn the directory /data/participant/demo you will find the files needed to run the nf-core/demo workflow, as demonstrated in the previous chapter. Now, you will run this workflow on the HPC using SLURM as the executor for your analysis steps.\nHere’s what you need to do:\n\nCreate a configuration file for running the pipeline, ensuring it includes the necessary settings for SLURM and respects the resource limits mentioned above.\n\nNote we’ve also created a directory to cache the Singularity images used by Nextflow in /data/participant/.nextflow-singularity-cache. Make sure to add this to your config.\n\nStart a screen or tmux session (your choice) to keep a persistent terminal running.\nEdit the script scripts/02-run_nfcore_demo.sh (e.g. using nano), adding the path to your configuration file to the Nextflow command with the -c option.\nLaunch the script using bash scripts/02-run_nfcore_demo.sh.\nDetach the screen/tmux session to return to your main terminal.\nCheck the queue with the squeue SLURM command to see if Nextflow has started submitting jobs.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nLogin to the HPC as instructed:\nssh participant@train.hpc\nCreate a configuration file in the /data/participant/demo directory, which we call trainhpc.config:\nprocess {\n    executor = 'slurm'\n    queue = 'normal'\n    resourceLimits = [\n      cpus: '8',\n      memory: '20.GB',\n      time: '24.h'\n    ]\n}\n\nexecutor {\n    queueSize = '100'\n    pollInterval = '2 min'\n    queueStatInterval = '5 min'\n    submitRateLimit = '50 sec'\n    exitReadTimeout = '5 min'\n}\n\nsingularity {\n    enabled = true\n    cacheDir = '/data/participant/.nextflow-singularity-cache'\n}\nStart a persistent terminal with either tmux new -s demo or screen -S demo.\nEdit the Nextflow script to include the configuration file using the -c option:\nnextflow run nf-core/demo \\\n  -profile \"singularity\" -revision \"1.1.0\" \\\n  -c \"trainhpc.config\" \\\n  --input \"samplesheet.csv\" \\\n  --outdir \"results/qc\" \\\n  --fasta \"genome/Mus_musculus.GRCm38.dna_sm.chr14.fa.gz\"\nDetach the screen/tmux session using:\n\nFor screen: press Ctrl + A then D\nFor tmux: press Ctrl + B then D\n\nFinally, use the squeue command to verify that Nextflow has started submitting jobs to the SLURM queue:\nsqueue -u participant\n\nYou should see your jobs listed in the queue, confirming that the workflow is running. You can check back on the progress of the workflow by re-attaching to your session with:\n\nscreen -r demo\ntmux attach -t demo",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Nextflow on HPC</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow_hpc.html#summary",
    "href": "materials/04-nextflow_hpc.html#summary",
    "title": "6  Nextflow on HPC",
    "section": "6.4 Summary",
    "text": "6.4 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nNextflow pipelines can be configured to run on a HPC using a custom config file. This file should include:\n\nWhich job scheduler is in use (e.g. slurm, lsf, etc.).\nThe queue/partition name that you want to run the jobs in.\nCPU and memory resource limits for that queue.\nJob submission settings to keep the load on the scheduler low.\n\nTo execute the workflow using the custom configuration file, use the -c your.config option with the nextflow command.\nThe nextflow process can be run on the login node, however it is recommended to use a terminal multiplexer (screen or tmux) to have persistent terminal that can be retrieved after logout.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Nextflow on HPC</span>"
    ]
  },
  {
    "objectID": "materials/05-advanced_config.html",
    "href": "materials/05-advanced_config.html",
    "title": "7  Advanced Nextflow configuration",
    "section": "",
    "text": "7.1 Nextflow configuration\nNextflow is a powerful platform designed to scale your analyses by taking full advantage of your system’s resources, allowing for controlled and scalable parallel processing. However, to take full advantage of these capabilities, pipeline configuration is often needed. You’ve already seen an example of this in the previous chapter covering the configuration on a HPC cluster that uses SLURM as a job scheduler.\nIn this section we cover further configuration options that can be used to run your Nextflow pipelines. According to the documentation, when a pipeline is launched, Nextflow searches for configuration files in several locations. Even if some of these files are not present, Nextflow still checks for them. Since these configuration files may contain conflicting settings, they are applied in the following order, from lowest to highest priority:\nWhen multiple configuration options are used, they are merged, so that the settings in the first override the same settings appearing in the second, and so on.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Nextflow configuration</span>"
    ]
  },
  {
    "objectID": "materials/05-advanced_config.html#nextflow-configuration",
    "href": "materials/05-advanced_config.html#nextflow-configuration",
    "title": "7  Advanced Nextflow configuration",
    "section": "",
    "text": "Parameters defined within the pipeline scripts (e.g. main.nf).\nThe configuration file located at $HOME/.nextflow/config.\nThe nextflow.config file in the project directory.\nThe nextflow.config file in the directory where the workflow is launched from.\nOne or more configuration files specified using the -c &lt;config-file&gt; option.\nParameters specified in a file using the -params-file option.\nParameters specified directly on the command line (--something value).",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Nextflow configuration</span>"
    ]
  },
  {
    "objectID": "materials/05-advanced_config.html#configuration-of-nf-core-pipelines",
    "href": "materials/05-advanced_config.html#configuration-of-nf-core-pipelines",
    "title": "7  Advanced Nextflow configuration",
    "section": "7.2 Configuration of nf-core pipelines",
    "text": "7.2 Configuration of nf-core pipelines\nAll nf-core pipelines have a default configuration file called nextflow.config, which can be found on their respective GitHub repositories. Looking at these files can feel quite daunting, but they are informative of the default settings that the pipelines runs with and give an indication of how you can change the behaviour of the pipeline to match your needs.\nAs an example, consider the nextflow.config file for nf-core/demo (open this file in a separate tab to compare with our notes).\nThe nextflow.config file generally follows a structured format, with several key components:\n\nDefault parameters (params{} scope): defines the default values for various parameters that the pipeline uses, such as input data, reference genomes, etc.\n\nYou may notice that several of these parameters match those specified in the documentation, and in fact they refer to the same thing. For example --input, --genome and --outdir.\n\nInclude statements (includeConfig): import other configuration files, which may set additional parameters or override defaults. For example:\n\nincludeConfig 'conf/base.config' imports the respective file found in the repository. This “base” configuration file sets a more dynamic resource allocation, depending on the computational demands of each task (more on this later).\nincludeConfig 'conf/igenomes.config' imports the igenomes configuration, where you can see the genome keywords available and the source of the respective files. (Remember, these are often outdated, so we recommend that you download your own genomes from an up-to-date source.)\n\nProfiles: pre-configured settings tailored for specific use cases, such as running on different computing environments or enabling debugging features.\n\nYou’ve already seen -profile singularity to manage software using Singularity containers.\nAll nf-core pipelines also have -profile test, which runs the pipeline through a test dataset. This can be useful if you want to check that everything is installed correct.\nMore than one profile can be used, for example nextflow run nf-core/demo -profile test,singularity.\n\nContainer registry settings: specifies where container images are downloaded from. The most common registry used is Red Hat’s quay.io.\nPlugins (plugins{} scope): plugins can be used to extend Nextflow’s functionality. This section defines which plugins are enabled in the pipeline.\n\nFor example, the nf-validation plugin can be used to validate the input from the user, such as pipeline parameters and sample sheets.\n\nEnvironment variables (env{} scope): define environment variables that will be exported to the environment where the workflow tasks are executed. For example, the defaults for R, Python and Julia in the nf-core/demo are shown below, but these can be used in your own configuration file.\nPYTHONNOUSERSITE = 1\nR_PROFILE_USER   = \"/.Rprofile\"\nR_ENVIRON_USER   = \"/.Renviron\"\nJULIA_DEPOT_PATH = \"/usr/local/share/julia\"\nShell directive (process.shell): define a custom shell command for process scripts.\n\nThe default setting is to use /bin/bash with options like -euo and pipefail, which captures exit codes when piping commands.\nAdvanced users may choose to change the shell or its behaviour.\n\nTracking and logging: log files include information about task execution, resource usage and errors. These are by default output to the directory pipeline_info.\n\nThese log files and reports are specified under the timeline{}, report{}, trace{} and dag{} scopes.\n\nManifest (manifest{} scope): metadata about the pipeline, including its name, author, description, main script, and version.\nCustom functions: custom functions can be included in the configuration to perform specific tasks, such as ensuring that resource requests do not exceed predefined limits.\n\nThese functions allow for a lot of flexibility to how pipelines run, in particular in managing the memory, time, and CPU resources, adjusting them based on the pipeline’s parameters and the system’s capabilities.\nFunctions are defined using the def directive.\n\n\n\n\n\n\n\n\nNoteGroovy language\n\n\n\nGroovy is a scripting language that is closely integrated with Java (on which Nextflow depends). The configuration files and pipeline scripts in Nextflow are written in Groovy, allowing users to define their own custom parameters, logic, and functions.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Nextflow configuration</span>"
    ]
  },
  {
    "objectID": "materials/05-advanced_config.html#process-selectors",
    "href": "materials/05-advanced_config.html#process-selectors",
    "title": "7  Advanced Nextflow configuration",
    "section": "7.3 Process selectors",
    "text": "7.3 Process selectors\nIn Nextflow, process selectors such as withLabel and withName are used to dynamically configure processes based on certain conditions, making workflows more flexible and adaptable. This feature allows you to apply specific configurations or actions to processes based on their names or labels.\n\n7.3.1 withLabel\nWhen developers write pipelines they can assign a label to each process of the pipeline. This is useful to indicate processes that share certain characteristics, such as being memory-intensive or CPU-intensive.\nFor nf-core pipelines, developers use the following standard labels:\n\n\n\nLabel Name\nCPUs\nMemory\nTime\n\n\n\n\nprocess_single\n1\n6GB\n4h\n\n\nprocess_low\n2\n12GB\n4h\n\n\nprocess_medium\n6\n36GB\n8h\n\n\nprocess_high\n12\n72GB\n16h\n\n\nprocess_long\n-\n-\n20h\n\n\nprocess_high_memory\n-\n200GB\n-\n\n\n\nThe withLabel selector allows you to apply a configuration to all processes that are annotated with a particular label. This is specified in the process scope, allowing you to easily invoke desired configurations by adding the label to your process.\nOn a HPC in particular, we may want to use different queues/partitions depending on whether a task requires high memory or not. As an example, for the Cascade Lake nodes at Cambridge you could set this configuration:\nprocess {\n    executor = 'slurm'\n    queue = 'cclake'\n    \n    withLabel: process_high_memory {\n        queue = 'cclake-himem'\n    }\n}\nThe above configuration would use the cclake partition for all processes, except those labelled process_high_memory, which would be submitted to the cclake-himem partition instead.\n\n\n7.3.2 withName\nIn the same manner, the withName selector can configure specific processes in your pipeline by its name. For example:\nprocess{\n    withName: 'MULTIQC'{\n            time    = { check_max( 2.h   * task.attempt, 'time' ) }\n            cpus    = { check_max( 4 * task.attempt, 'cpus' ) }\n            memory  = { check_max( 16.GB * task.attempt, 'memory')}\n        }\n    withLabel:error_retry {\n        errorStrategy = 'retry'\n        maxRetries    = 3\n    }\n}\nHere, we target the MultiQC step of a workflow specifically and define the resources to be used by that task. In this example, we calculate a value dynamically, such that if the task fails (e.g. due to an out-of-memory error), it will be resubmitted but this time multiplying the initial values by 2 (2nd task attempt) and then 3 (3rd attempt) and so on.\nThe maximum number of times a task is retried is defined in the withLabel:error_retry {} directive, also included in the configuration.\n\n\n7.3.3 Selector expressions\nBoth withLabel and withName selectors allow the use of a regular expression in order to apply the same configuration to all processes matching the specified pattern. For example:\nprocess{\n    withName: 'MULTIQC|FASTQC'{\n            time    = { check_max( 2.h   * task.attempt, 'time' ) }\n            cpus    = { check_max( 4 * task.attempt, 'cpus' ) }\n            memory  = { check_max( 16.GB * task.attempt, 'memory')}\n        }\n}\nIn this case the configuration would apply to processes called “MULTIQC” or “FASTQC”. See more info and examples in the Nextflow selector expressions documentation.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Nextflow configuration</span>"
    ]
  },
  {
    "objectID": "materials/05-advanced_config.html#custom-parameters",
    "href": "materials/05-advanced_config.html#custom-parameters",
    "title": "7  Advanced Nextflow configuration",
    "section": "7.4 Custom parameters",
    "text": "7.4 Custom parameters\nThe params scope defines parameters that will be accessible in the pipeline script as --name-of-your-parameter. To achieve this you can prefix the parameter names with params or surround them by curly brackets.\nFor example, let’s say you wanted to define two options:\n\nSelect the queue/partition name that jobs get submitted to on your HPC.\nSelect the number of times Nextflow should retry to run a task in case of failure (e.g. due to low resources).\n\n// define custom parameters\nparams {\n  partition = 'cclake' // we use \"cclake\" as a default value\n  n_retries = 2        // we use 2 as a default value\n}\n\n// define options to run the processes\nprocess{\n    executor = 'slurm'\n    queue = params.partition // our custom parameter\n    \n    withLabel:error_retry {\n        errorStrategy = 'retry'\n        maxRetries    = params.n_retries // our custom parameter\n    }\n}\nNote that instead of the params {} scope, we could have also defined the parameters like this:\nparams.partition = 'cclake'\nparams.n_retries = 2\nEither way, having this as a configuration file, would now allow you to run the pipeline with these two options For example:\nnextflow -c your_custom.config -profile singularity nf-core/demo \\\n  --partition 'icelake' \\\n  --n_retries 3 \\\n  ...etc...",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Nextflow configuration</span>"
    ]
  },
  {
    "objectID": "materials/05-advanced_config.html#institutional-configuration",
    "href": "materials/05-advanced_config.html#institutional-configuration",
    "title": "7  Advanced Nextflow configuration",
    "section": "7.5 Institutional configuration",
    "text": "7.5 Institutional configuration\nThe nf-core community has developed a set of institutional configuration files, which can be used using the -profile option. These are listed at nf-co.re/configs and include documentation of how to set up Nextflow on each specific institution.\nFor example, at Cambridge University, you could run your pipelines with the option -profile singularity,cambridge to use the default Cambridge configuration.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Nextflow configuration</span>"
    ]
  },
  {
    "objectID": "materials/05-advanced_config.html#summary",
    "href": "materials/05-advanced_config.html#summary",
    "title": "7  Advanced Nextflow configuration",
    "section": "7.6 Summary",
    "text": "7.6 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nNextflow configuration files define the behaviour of the pipeline run. Nextflow always looks for default files in specific locations (documentation), and the user may also define other configuration files and use them with the -c option.\nPipelines can be extensively customised: parameters can be defined, resource allocation can be customised to the environment or task at hand, and environment variables defined.\nThe withLabel and withName directives can be used to tailor resource usage for specific tasks or groups of tasks, such as high-memory or CPU-heavy tasks. This could be used, for example, to submit different kinds of tasks to different queues/partitions on a HPC.\nUsers can define new parameters to the pipeline, which may be useful to adjust the behaviour of the pipeline in different environments, such as a HPC.",
    "crumbs": [
      "Slides",
      "Pipelines",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Nextflow configuration</span>"
    ]
  }
]